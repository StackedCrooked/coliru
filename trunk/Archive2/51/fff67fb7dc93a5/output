	.file	"main.cpp"
	.section	.text._ZN5boost6random19normal_distributionIdEclINS0_23mersenne_twister_engineIjLm32ELm624ELm397ELm31ELj2567483615ELm11ELj4294967295ELm7ELj2636928640ELm15ELj4022730752ELm18ELj1812433253EEEEEdRT_,"axG",@progbits,_ZN5boost6random19normal_distributionIdEclINS0_23mersenne_twister_engineIjLm32ELm624ELm397ELm31ELj2567483615ELm11ELj4294967295ELm7ELj2636928640ELm15ELj4022730752ELm18ELj1812433253EEEEEdRT_,comdat
	.align 2
	.p2align 5,,31
	.weak	_ZN5boost6random19normal_distributionIdEclINS0_23mersenne_twister_engineIjLm32ELm624ELm397ELm31ELj2567483615ELm11ELj4294967295ELm7ELj2636928640ELm15ELj4022730752ELm18ELj1812433253EEEEEdRT_
	.type	_ZN5boost6random19normal_distributionIdEclINS0_23mersenne_twister_engineIjLm32ELm624ELm397ELm31ELj2567483615ELm11ELj4294967295ELm7ELj2636928640ELm15ELj4022730752ELm18ELj1812433253EEEEEdRT_, @function
_ZN5boost6random19normal_distributionIdEclINS0_23mersenne_twister_engineIjLm32ELm624ELm397ELm31ELj2567483615ELm11ELj4294967295ELm7ELj2636928640ELm15ELj4022730752ELm18ELj1812433253EEEEEdRT_:
.LFB9145:
	.cfi_startproc
	pushq	%r15
	.cfi_def_cfa_offset 16
	.cfi_offset 15, -16
	pushq	%r14
	.cfi_def_cfa_offset 24
	.cfi_offset 14, -24
	pushq	%r13
	.cfi_def_cfa_offset 32
	.cfi_offset 13, -32
	pushq	%r12
	.cfi_def_cfa_offset 40
	.cfi_offset 12, -40
	pushq	%rbp
	.cfi_def_cfa_offset 48
	.cfi_offset 6, -48
	pushq	%rbx
	.cfi_def_cfa_offset 56
	.cfi_offset 3, -56
	movq	%rdi, %rbx
	subq	$120, %rsp
	.cfi_def_cfa_offset 176
	cmpb	$0, 40(%rdi)
	jne	.L2
	movq	%rsi, %r14
	movq	$222, 24(%rsp)
	leaq	908(%rsi), %r13
	andl	$15, %r14d
	andl	$15, %r13d
	movq	$396, 16(%rsp)
	shrq	$2, %r14
	shrq	$2, %r13
	movq	2496(%rsi), %r8
	negq	%r14
	negq	%r13
	vmovsd	.LC4(%rip), %xmm6
	andl	$3, %r14d
	subq	%r14, 24(%rsp)
	andl	$3, %r13d
	movq	24(%rsp), %rdx
	subq	%r13, 16(%rsp)
	leaq	908(,%r13,4), %rcx
	vmovsd	.LC5(%rip), %xmm5
	vmovdqa	.LC0(%rip), %xmm3
	vmovdqa	.LC1(%rip), %xmm2
	shrq	$2, %rdx
	leaq	0(,%rdx,4), %rax
	leaq	-3(%rdx), %r15
	vmovdqa	.LC2(%rip), %xmm1
	movq	%rax, 80(%rsp)
	leaq	0(,%r14,4), %rax
	vmovdqa	.LC3(%rip), %xmm0
	leaq	(%rsi,%rax), %rdi
	movq	%rdi, 40(%rsp)
	leaq	4(%rsi,%rax), %rdi
	leaq	1588(%rsi,%rax), %rax
	movq	%rax, 56(%rsp)
	movq	16(%rsp), %rax
	movq	%rdi, 48(%rsp)
	shrq	$2, %rax
	leaq	0(,%rax,4), %rdi
	movq	%rdi, 72(%rsp)
	leaq	(%rsi,%rcx), %rdi
	movq	%rdi, 64(%rsp)
	leaq	4(%rsi,%rcx), %rdi
	movq	%rdi, 32(%rsp)
	leaq	-908(%rcx), %rdi
	movq	%rdi, 96(%rsp)
	addq	%rsi, %rdi
	movq	%rdi, 88(%rsp)
	leaq	-3(%rax), %rdi
	movq	%rdi, 8(%rsp)
	jmp	.L26
	.p2align 5,,7
	.p2align 3
.L85:
	movl	(%rsi,%r8,4), %ecx
	incq	%r8
.L4:
	movl	%ecx, %edi
	movq	%r8, 2496(%rsi)
	shrl	$11, %edi
	xorl	%ecx, %edi
	movl	%edi, %ecx
	sall	$7, %ecx
	andl	$-1658038656, %ecx
	xorl	%edi, %ecx
	movl	%ecx, %edi
	sall	$15, %edi
	andl	$-272236544, %edi
	xorl	%ecx, %edi
	movl	%edi, %ecx
	shrl	$18, %ecx
	xorl	%ecx, %edi
	movq	%rdi, 104(%rsp)
	vcvtsi2sdq	104(%rsp), %xmm4, %xmm4
	vmulsd	%xmm6, %xmm4, %xmm4
	vucomisd	%xmm4, %xmm5
	ja	.L84
.L26:
	cmpq	$624, %r8
	jne	.L85
	testq	%r14, %r14
	je	.L54
	movl	4(%rsi), %edi
	movl	(%rsi), %ecx
	movl	%edi, %r8d
	movl	%edi, %r9d
	andl	$-2147483648, %ecx
	andl	$1, %r8d
	andl	$2147483647, %r9d
	imull	$-1727483681, %r8d, %r8d
	xorl	1588(%rsi), %r8d
	orl	%r9d, %ecx
	shrl	%ecx
	xorl	%ecx, %r8d
	cmpq	$1, %r14
	movl	%r8d, (%rsi)
	je	.L55
	movl	8(%rsi), %r8d
	andl	$-2147483648, %edi
	movl	%r8d, %ecx
	movl	%r8d, %r9d
	andl	$1, %ecx
	andl	$2147483647, %r9d
	imull	$-1727483681, %ecx, %ecx
	xorl	1592(%rsi), %ecx
	orl	%r9d, %edi
	shrl	%edi
	xorl	%edi, %ecx
	cmpq	$3, %r14
	movl	%ecx, 4(%rsi)
	jne	.L56
	movl	12(%rsi), %edi
	andl	$-2147483648, %r8d
	movl	$219, %ebp
	movl	$3, %r12d
	movl	%edi, %ecx
	andl	$2147483647, %edi
	andl	$1, %ecx
	orl	%edi, %r8d
	imull	$-1727483681, %ecx, %ecx
	xorl	1596(%rsi), %ecx
	shrl	%r8d
	xorl	%r8d, %ecx
	movl	%ecx, 8(%rsi)
.L5:
	movq	40(%rsp), %rcx
	movq	56(%rsp), %r8
	xorl	%r10d, %r10d
	movq	48(%rsp), %rdi
	movq	%rcx, %r9
	jmp	.L11
	.p2align 5,,7
	.p2align 3
.L7:
	movq	%r11, %r10
.L11:
	vmovdqu	(%rdi), %xmm4
	vmovdqu	(%r8), %xmm7
	prefetcht0	176(%rcx)
	prefetcht0	176(%rdi)
	prefetcht0	176(%r8)
	leaq	4(%r10), %r11
	vpand	%xmm3, %xmm4, %xmm8
	vpand	%xmm1, %xmm4, %xmm4
	addq	$5, %r10
	prefetchw	176(%r9)
	addq	$64, %rcx
	addq	$64, %rdi
	vpmulld	%xmm2, %xmm8, %xmm8
	vpxor	%xmm7, %xmm8, %xmm7
	vpand	-64(%rcx), %xmm0, %xmm8
	addq	$64, %r8
	addq	$64, %r9
	vpor	%xmm8, %xmm4, %xmm4
	vpsrld	$1, %xmm4, %xmm4
	vpxor	%xmm4, %xmm7, %xmm4
	vmovaps	%xmm4, -64(%r9)
	vmovdqu	-48(%rdi), %xmm4
	vmovdqu	-48(%r8), %xmm7
	vpand	%xmm3, %xmm4, %xmm8
	vpand	%xmm1, %xmm4, %xmm4
	vpmulld	%xmm2, %xmm8, %xmm8
	vpxor	%xmm7, %xmm8, %xmm7
	vpand	-48(%rcx), %xmm0, %xmm8
	vpor	%xmm8, %xmm4, %xmm4
	vpsrld	$1, %xmm4, %xmm4
	vpxor	%xmm4, %xmm7, %xmm4
	vmovaps	%xmm4, -48(%r9)
	vmovdqu	-32(%rdi), %xmm4
	vmovdqu	-32(%r8), %xmm7
	vpand	%xmm3, %xmm4, %xmm8
	vpand	%xmm1, %xmm4, %xmm4
	vpmulld	%xmm2, %xmm8, %xmm8
	vpxor	%xmm7, %xmm8, %xmm7
	vpand	-32(%rcx), %xmm0, %xmm8
	vpor	%xmm8, %xmm4, %xmm4
	vpsrld	$1, %xmm4, %xmm4
	vpxor	%xmm4, %xmm7, %xmm4
	vmovaps	%xmm4, -32(%r9)
	vmovdqu	-16(%rdi), %xmm4
	vmovdqu	-16(%r8), %xmm7
	vpand	%xmm3, %xmm4, %xmm8
	vpand	%xmm1, %xmm4, %xmm4
	vpmulld	%xmm2, %xmm8, %xmm8
	vpxor	%xmm7, %xmm8, %xmm7
	vpand	-16(%rcx), %xmm0, %xmm8
	vpor	%xmm8, %xmm4, %xmm4
	vpsrld	$1, %xmm4, %xmm4
	vpxor	%xmm4, %xmm7, %xmm4
	vmovaps	%xmm4, -16(%r9)
	cmpq	%r15, %r10
	jb	.L7
	xorl	%r10d, %r10d
	.p2align 5,,24
	.p2align 3
.L13:
	vmovdqu	(%rdi,%r10), %xmm4
	vmovdqu	(%r8,%r10), %xmm8
	incq	%r11
	vpand	%xmm3, %xmm4, %xmm7
	vpand	%xmm1, %xmm4, %xmm4
	vpmulld	%xmm2, %xmm7, %xmm7
	vpxor	%xmm8, %xmm7, %xmm7
	vpand	(%rcx,%r10), %xmm0, %xmm8
	vpor	%xmm8, %xmm4, %xmm4
	vpsrld	$1, %xmm4, %xmm4
	vpxor	%xmm4, %xmm7, %xmm4
	vmovaps	%xmm4, (%r9,%r10)
	addq	$16, %r10
	cmpq	%rdx, %r11
	jb	.L13
	movq	80(%rsp), %rdi
	addq	%rdi, %r12
	subq	%rdi, %rbp
	cmpq	%rdi, 24(%rsp)
	je	.L9
	leaq	1(%r12), %r9
	leaq	(%rsi,%r12,4), %r11
	movl	(%rsi,%r9,4), %r8d
	movl	(%r11), %r10d
	movl	%r8d, %ecx
	movl	%r8d, %edi
	andl	$-2147483648, %r10d
	andl	$1, %ecx
	andl	$2147483647, %edi
	imull	$-1727483681, %ecx, %ecx
	xorl	1588(%rsi,%r12,4), %ecx
	orl	%r10d, %edi
	shrl	%edi
	xorl	%edi, %ecx
	cmpq	$1, %rbp
	movl	%ecx, (%r11)
	je	.L9
	leaq	2(%r12), %r11
	andl	$-2147483648, %r8d
	movl	(%rsi,%r11,4), %r10d
	movl	%r10d, %ecx
	movl	%r10d, %edi
	andl	$1, %ecx
	andl	$2147483647, %edi
	imull	$-1727483681, %ecx, %ecx
	xorl	1592(%rsi,%r12,4), %ecx
	orl	%r8d, %edi
	shrl	%edi
	xorl	%edi, %ecx
	cmpq	$2, %rbp
	movl	%ecx, (%rsi,%r9,4)
	je	.L9
	movl	12(%rsi,%r12,4), %edi
	andl	$-2147483648, %r10d
	movl	%edi, %ecx
	andl	$2147483647, %edi
	andl	$1, %ecx
	orl	%r10d, %edi
	imull	$-1727483681, %ecx, %ecx
	xorl	1596(%rsi,%r12,4), %ecx
	shrl	%edi
	xorl	%edi, %ecx
	movl	%ecx, (%rsi,%r11,4)
.L9:
	movl	892(%rsi), %r9d
	movl	888(%rsi), %r8d
	movl	%r9d, %ecx
	movl	%r9d, %edi
	andl	$-2147483648, %r8d
	andl	$1, %ecx
	andl	$2147483647, %edi
	andl	$-2147483648, %r9d
	imull	$-1727483681, %ecx, %ecx
	xorl	2476(%rsi), %ecx
	orl	%r8d, %edi
	movl	896(%rsi), %r8d
	shrl	%edi
	xorl	%edi, %ecx
	movl	%r8d, %edi
	movl	%ecx, 888(%rsi)
	movl	%r8d, %ecx
	andl	$2147483647, %edi
	andl	$1, %ecx
	orl	%r9d, %edi
	movl	900(%rsi), %r9d
	imull	$-1727483681, %ecx, %ecx
	xorl	2480(%rsi), %ecx
	shrl	%edi
	andl	$-2147483648, %r8d
	xorl	%edi, %ecx
	movl	%r9d, %edi
	movl	%ecx, 892(%rsi)
	movl	%r9d, %ecx
	andl	$2147483647, %edi
	andl	$1, %ecx
	orl	%r8d, %edi
	movl	904(%rsi), %r8d
	imull	$-1727483681, %ecx, %ecx
	xorl	2484(%rsi), %ecx
	shrl	%edi
	andl	$-2147483648, %r9d
	xorl	%edi, %ecx
	movl	%r8d, %edi
	movl	%ecx, 896(%rsi)
	movl	%r8d, %ecx
	andl	$2147483647, %edi
	andl	$1, %ecx
	orl	%r9d, %edi
	movl	908(%rsi), %r9d
	imull	$-1727483681, %ecx, %ecx
	xorl	2488(%rsi), %ecx
	shrl	%edi
	xorl	%edi, %ecx
	movl	%r9d, %edi
	movl	%ecx, 900(%rsi)
	movl	%r9d, %ecx
	andl	$1, %ecx
	imull	$-1727483681, %ecx, %ecx
	xorl	2492(%rsi), %ecx
	andl	$2147483647, %edi
	andl	$-2147483648, %r8d
	orl	%r8d, %edi
	shrl	%edi
	xorl	%edi, %ecx
	testq	%r13, %r13
	movl	%ecx, 904(%rsi)
	je	.L57
	movl	912(%rsi), %edi
	andl	$-2147483648, %r9d
	movl	%edi, %ecx
	movl	%edi, %r8d
	andl	$1, %ecx
	andl	$2147483647, %r8d
	imull	$-1727483681, %ecx, %ecx
	xorl	(%rsi), %ecx
	orl	%r8d, %r9d
	shrl	%r9d
	xorl	%r9d, %ecx
	cmpq	$1, %r13
	movl	%ecx, 908(%rsi)
	je	.L58
	movl	916(%rsi), %r8d
	andl	$-2147483648, %edi
	movl	%r8d, %ecx
	movl	%r8d, %r9d
	andl	$1, %ecx
	andl	$2147483647, %r9d
	imull	$-1727483681, %ecx, %ecx
	xorl	4(%rsi), %ecx
	orl	%r9d, %edi
	shrl	%edi
	xorl	%edi, %ecx
	cmpq	$3, %r13
	movl	%ecx, 912(%rsi)
	jne	.L59
	movl	920(%rsi), %edi
	andl	$-2147483648, %r8d
	movl	$393, %ebp
	movl	$230, %r12d
	movl	%edi, %ecx
	andl	$2147483647, %edi
	andl	$1, %ecx
	orl	%edi, %r8d
	imull	$-1727483681, %ecx, %ecx
	xorl	8(%rsi), %ecx
	shrl	%r8d
	xorl	%r8d, %ecx
	movl	%ecx, 916(%rsi)
.L14:
	movq	64(%rsp), %rcx
	movq	88(%rsp), %r8
	xorl	%r10d, %r10d
	movq	32(%rsp), %rdi
	movq	%rcx, %r9
	jmp	.L20
	.p2align 5,,7
	.p2align 3
.L16:
	movq	%r11, %r10
.L20:
	vmovdqu	(%rdi), %xmm4
	vmovdqu	(%r8), %xmm7
	prefetcht0	176(%rcx)
	prefetcht0	176(%rdi)
	prefetcht0	176(%r8)
	prefetchw	176(%r9)
	vpand	%xmm3, %xmm4, %xmm8
	vpand	%xmm1, %xmm4, %xmm4
	addq	$64, %rcx
	addq	$64, %rdi
	addq	$64, %r8
	addq	$64, %r9
	vpmulld	%xmm2, %xmm8, %xmm8
	vpxor	%xmm7, %xmm8, %xmm7
	vpand	-64(%rcx), %xmm0, %xmm8
	leaq	4(%r10), %r11
	addq	$5, %r10
	vpor	%xmm8, %xmm4, %xmm4
	vpsrld	$1, %xmm4, %xmm4
	vpxor	%xmm4, %xmm7, %xmm4
	vmovaps	%xmm4, -64(%r9)
	vmovdqu	-48(%rdi), %xmm4
	vmovdqu	-48(%r8), %xmm7
	vpand	%xmm3, %xmm4, %xmm8
	vpand	%xmm1, %xmm4, %xmm4
	vpmulld	%xmm2, %xmm8, %xmm8
	vpxor	%xmm7, %xmm8, %xmm7
	vpand	-48(%rcx), %xmm0, %xmm8
	vpor	%xmm8, %xmm4, %xmm4
	vpsrld	$1, %xmm4, %xmm4
	vpxor	%xmm4, %xmm7, %xmm4
	vmovaps	%xmm4, -48(%r9)
	vmovdqu	-32(%rdi), %xmm4
	vmovdqu	-32(%r8), %xmm7
	vpand	%xmm3, %xmm4, %xmm8
	vpand	%xmm1, %xmm4, %xmm4
	vpmulld	%xmm2, %xmm8, %xmm8
	vpxor	%xmm7, %xmm8, %xmm7
	vpand	-32(%rcx), %xmm0, %xmm8
	vpor	%xmm8, %xmm4, %xmm4
	vpsrld	$1, %xmm4, %xmm4
	vpxor	%xmm4, %xmm7, %xmm4
	vmovaps	%xmm4, -32(%r9)
	vmovdqu	-16(%rdi), %xmm4
	vmovdqu	-16(%r8), %xmm7
	vpand	%xmm3, %xmm4, %xmm8
	vpand	%xmm1, %xmm4, %xmm4
	vpmulld	%xmm2, %xmm8, %xmm8
	vpxor	%xmm7, %xmm8, %xmm7
	vpand	-16(%rcx), %xmm0, %xmm8
	vpor	%xmm8, %xmm4, %xmm4
	vpsrld	$1, %xmm4, %xmm4
	vpxor	%xmm4, %xmm7, %xmm4
	vmovaps	%xmm4, -16(%r9)
	cmpq	8(%rsp), %r10
	jb	.L16
	xorl	%r10d, %r10d
	.p2align 5,,24
	.p2align 3
.L22:
	vmovdqu	(%rdi,%r10), %xmm4
	vmovdqu	(%r8,%r10), %xmm8
	incq	%r11
	vpand	%xmm3, %xmm4, %xmm7
	vpand	%xmm1, %xmm4, %xmm4
	vpmulld	%xmm2, %xmm7, %xmm7
	vpxor	%xmm8, %xmm7, %xmm7
	vpand	(%rcx,%r10), %xmm0, %xmm8
	vpor	%xmm8, %xmm4, %xmm4
	vpsrld	$1, %xmm4, %xmm4
	vpxor	%xmm4, %xmm7, %xmm4
	vmovaps	%xmm4, (%r9,%r10)
	addq	$16, %r10
	cmpq	%rax, %r11
	jb	.L22
	movq	72(%rsp), %rdi
	addq	%rdi, %r12
	subq	%rdi, %rbp
	cmpq	%rdi, 16(%rsp)
	je	.L18
	leaq	1(%r12), %r9
	leaq	(%rsi,%r12,4), %r11
	movl	(%rsi,%r9,4), %r8d
	movl	(%r11), %r10d
	movl	%r8d, %ecx
	movl	%r8d, %edi
	andl	$-2147483648, %r10d
	andl	$1, %ecx
	andl	$2147483647, %edi
	imull	$-1727483681, %ecx, %ecx
	xorl	-908(%rsi,%r12,4), %ecx
	orl	%r10d, %edi
	shrl	%edi
	xorl	%edi, %ecx
	cmpq	$1, %rbp
	movl	%ecx, (%r11)
	je	.L18
	leaq	2(%r12), %r11
	andl	$-2147483648, %r8d
	movl	(%rsi,%r11,4), %r10d
	movl	%r10d, %ecx
	movl	%r10d, %edi
	andl	$1, %ecx
	andl	$2147483647, %edi
	imull	$-1727483681, %ecx, %ecx
	xorl	-904(%rsi,%r12,4), %ecx
	orl	%r8d, %edi
	shrl	%edi
	xorl	%edi, %ecx
	cmpq	$2, %rbp
	movl	%ecx, (%rsi,%r9,4)
	je	.L18
	movl	12(%rsi,%r12,4), %edi
	andl	$-2147483648, %r10d
	movl	%edi, %ecx
	andl	$2147483647, %edi
	andl	$1, %ecx
	orl	%r10d, %edi
	imull	$-1727483681, %ecx, %ecx
	xorl	-900(%rsi,%r12,4), %ecx
	shrl	%edi
	xorl	%edi, %ecx
	movl	%ecx, (%rsi,%r11,4)
.L18:
	movl	(%rsi), %ecx
	movl	2492(%rsi), %r9d
	movl	%ecx, %edi
	movl	%ecx, %r8d
	andl	$-2147483648, %r9d
	andl	$1, %edi
	andl	$2147483647, %r8d
	imull	$-1727483681, %edi, %edi
	xorl	1584(%rsi), %edi
	orl	%r9d, %r8d
	shrl	%r8d
	xorl	%r8d, %edi
	movl	$1, %r8d
	movl	%edi, 2492(%rsi)
	jmp	.L4
	.p2align 5,,7
	.p2align 3
.L2:
	vmovsd	.LC7(%rip), %xmm0
	vmovsd	32(%rdi), %xmm1
	movb	$0, 40(%rdi)
	vmulsd	16(%rdi), %xmm0, %xmm0
	vmovsd	%xmm1, 8(%rsp)
	call	sin
	vmovsd	8(%rsp), %xmm1
.L53:
	vmulsd	%xmm0, %xmm1, %xmm0
	vmulsd	8(%rbx), %xmm0, %xmm0
	vaddsd	(%rbx), %xmm0, %xmm0
	addq	$120, %rsp
	.cfi_remember_state
	.cfi_def_cfa_offset 56
	popq	%rbx
	.cfi_def_cfa_offset 48
	popq	%rbp
	.cfi_def_cfa_offset 40
	popq	%r12
	.cfi_def_cfa_offset 32
	popq	%r13
	.cfi_def_cfa_offset 24
	popq	%r14
	.cfi_def_cfa_offset 16
	popq	%r15
	.cfi_def_cfa_offset 8
	ret
	.p2align 5,,7
	.p2align 3
.L84:
	.cfi_restore_state
	movq	96(%rsp), %rdi
	vmovsd	%xmm4, 16(%rbx)
	movq	%rbx, 96(%rsp)
	vmovdqa	.LC0(%rip), %xmm3
	vmovdqa	.LC1(%rip), %xmm2
	vmovdqa	.LC2(%rip), %xmm1
	addq	%rsi, %rdi
	vmovdqa	.LC3(%rip), %xmm0
	movq	8(%rsp), %rbx
	movq	%rdi, 88(%rsp)
	jmp	.L50
	.p2align 5,,7
	.p2align 3
.L87:
	movl	(%rsi,%r8,4), %ecx
	incq	%r8
.L28:
	movl	%ecx, %edi
	movq	%r8, 2496(%rsi)
	shrl	$11, %edi
	xorl	%ecx, %edi
	movl	%edi, %ecx
	sall	$7, %ecx
	andl	$-1658038656, %ecx
	xorl	%edi, %ecx
	movl	%ecx, %edi
	sall	$15, %edi
	andl	$-272236544, %edi
	xorl	%ecx, %edi
	movl	%edi, %ecx
	shrl	$18, %ecx
	xorl	%ecx, %edi
	movq	%rdi, 104(%rsp)
	vcvtsi2sdq	104(%rsp), %xmm4, %xmm4
	vmulsd	%xmm6, %xmm4, %xmm4
	vucomisd	%xmm4, %xmm5
	ja	.L86
.L50:
	cmpq	$624, %r8
	jne	.L87
	testq	%r14, %r14
	je	.L60
	movl	4(%rsi), %edi
	movl	(%rsi), %ecx
	movl	%edi, %r8d
	movl	%edi, %r9d
	andl	$-2147483648, %ecx
	andl	$1, %r8d
	andl	$2147483647, %r9d
	imull	$-1727483681, %r8d, %r8d
	xorl	1588(%rsi), %r8d
	orl	%r9d, %ecx
	shrl	%ecx
	xorl	%ecx, %r8d
	cmpq	$1, %r14
	movl	%r8d, (%rsi)
	je	.L61
	movl	8(%rsi), %r8d
	andl	$-2147483648, %edi
	movl	%r8d, %ecx
	movl	%r8d, %r9d
	andl	$1, %ecx
	andl	$2147483647, %r9d
	imull	$-1727483681, %ecx, %ecx
	xorl	1592(%rsi), %ecx
	orl	%r9d, %edi
	shrl	%edi
	xorl	%edi, %ecx
	cmpq	$3, %r14
	movl	%ecx, 4(%rsi)
	jne	.L62
	movl	12(%rsi), %edi
	andl	$-2147483648, %r8d
	movl	$219, %r12d
	movl	$3, %ebp
	movl	%edi, %ecx
	andl	$2147483647, %edi
	andl	$1, %ecx
	orl	%edi, %r8d
	imull	$-1727483681, %ecx, %ecx
	xorl	1596(%rsi), %ecx
	shrl	%r8d
	xorl	%r8d, %ecx
	movl	%ecx, 8(%rsi)
.L29:
	movq	40(%rsp), %rcx
	movq	56(%rsp), %r8
	xorl	%r10d, %r10d
	movq	48(%rsp), %rdi
	movq	%rcx, %r9
	jmp	.L35
	.p2align 5,,7
	.p2align 3
.L31:
	movq	%r11, %r10
.L35:
	vmovdqu	(%rdi), %xmm4
	vmovdqu	(%r8), %xmm7
	prefetcht0	176(%rcx)
	prefetcht0	176(%rdi)
	prefetcht0	176(%r8)
	leaq	4(%r10), %r11
	vpand	%xmm3, %xmm4, %xmm8
	vpand	%xmm1, %xmm4, %xmm4
	addq	$5, %r10
	prefetchw	176(%r9)
	addq	$64, %rcx
	addq	$64, %rdi
	vpmulld	%xmm2, %xmm8, %xmm8
	vpxor	%xmm7, %xmm8, %xmm7
	vpand	-64(%rcx), %xmm0, %xmm8
	addq	$64, %r8
	addq	$64, %r9
	vpor	%xmm4, %xmm8, %xmm4
	vpsrld	$1, %xmm4, %xmm4
	vpxor	%xmm4, %xmm7, %xmm4
	vmovaps	%xmm4, -64(%r9)
	vmovdqu	-48(%rdi), %xmm4
	vmovdqu	-48(%r8), %xmm7
	vpand	%xmm3, %xmm4, %xmm8
	vpand	%xmm1, %xmm4, %xmm4
	vpmulld	%xmm2, %xmm8, %xmm8
	vpxor	%xmm7, %xmm8, %xmm7
	vpand	-48(%rcx), %xmm0, %xmm8
	vpor	%xmm8, %xmm4, %xmm4
	vpsrld	$1, %xmm4, %xmm4
	vpxor	%xmm4, %xmm7, %xmm4
	vmovaps	%xmm4, -48(%r9)
	vmovdqu	-32(%rdi), %xmm4
	vmovdqu	-32(%r8), %xmm7
	vpand	%xmm3, %xmm4, %xmm8
	vpand	%xmm1, %xmm4, %xmm4
	vpmulld	%xmm2, %xmm8, %xmm8
	vpxor	%xmm7, %xmm8, %xmm7
	vpand	-32(%rcx), %xmm0, %xmm8
	vpor	%xmm8, %xmm4, %xmm4
	vpsrld	$1, %xmm4, %xmm4
	vpxor	%xmm4, %xmm7, %xmm4
	vmovaps	%xmm4, -32(%r9)
	vmovdqu	-16(%rdi), %xmm4
	vmovdqu	-16(%r8), %xmm7
	vpand	%xmm3, %xmm4, %xmm8
	vpand	%xmm1, %xmm4, %xmm4
	vpmulld	%xmm2, %xmm8, %xmm8
	vpxor	%xmm7, %xmm8, %xmm7
	vpand	-16(%rcx), %xmm0, %xmm8
	vpor	%xmm8, %xmm4, %xmm4
	vpsrld	$1, %xmm4, %xmm4
	vpxor	%xmm4, %xmm7, %xmm4
	vmovaps	%xmm4, -16(%r9)
	cmpq	%r15, %r10
	jb	.L31
	xorl	%r10d, %r10d
	.p2align 5,,24
	.p2align 3
.L37:
	vmovdqu	(%rdi,%r10), %xmm4
	vmovdqu	(%r8,%r10), %xmm8
	incq	%r11
	vpand	%xmm3, %xmm4, %xmm7
	vpand	%xmm1, %xmm4, %xmm4
	vpmulld	%xmm2, %xmm7, %xmm7
	vpxor	%xmm8, %xmm7, %xmm7
	vpand	(%rcx,%r10), %xmm0, %xmm8
	vpor	%xmm4, %xmm8, %xmm4
	vpsrld	$1, %xmm4, %xmm4
	vpxor	%xmm4, %xmm7, %xmm4
	vmovaps	%xmm4, (%r9,%r10)
	addq	$16, %r10
	cmpq	%rdx, %r11
	jb	.L37
	movq	80(%rsp), %rdi
	addq	%rdi, %rbp
	subq	%rdi, %r12
	cmpq	%rdi, 24(%rsp)
	je	.L33
	leaq	1(%rbp), %r9
	leaq	(%rsi,%rbp,4), %r11
	movl	(%rsi,%r9,4), %r8d
	movl	(%r11), %r10d
	movl	%r8d, %ecx
	movl	%r8d, %edi
	andl	$-2147483648, %r10d
	andl	$1, %ecx
	andl	$2147483647, %edi
	imull	$-1727483681, %ecx, %ecx
	xorl	1588(%rsi,%rbp,4), %ecx
	orl	%r10d, %edi
	shrl	%edi
	xorl	%edi, %ecx
	cmpq	$1, %r12
	movl	%ecx, (%r11)
	je	.L33
	leaq	2(%rbp), %r11
	andl	$-2147483648, %r8d
	movl	(%rsi,%r11,4), %r10d
	movl	%r10d, %ecx
	movl	%r10d, %edi
	andl	$1, %ecx
	andl	$2147483647, %edi
	imull	$-1727483681, %ecx, %ecx
	xorl	1592(%rsi,%rbp,4), %ecx
	orl	%r8d, %edi
	shrl	%edi
	xorl	%edi, %ecx
	cmpq	$2, %r12
	movl	%ecx, (%rsi,%r9,4)
	je	.L33
	movl	12(%rsi,%rbp,4), %edi
	andl	$-2147483648, %r10d
	movl	%edi, %ecx
	andl	$2147483647, %edi
	andl	$1, %ecx
	orl	%r10d, %edi
	imull	$-1727483681, %ecx, %ecx
	xorl	1596(%rsi,%rbp,4), %ecx
	shrl	%edi
	xorl	%edi, %ecx
	movl	%ecx, (%rsi,%r11,4)
.L33:
	movl	892(%rsi), %r9d
	movl	888(%rsi), %r8d
	movl	%r9d, %ecx
	movl	%r9d, %edi
	andl	$-2147483648, %r8d
	andl	$1, %ecx
	andl	$2147483647, %edi
	andl	$-2147483648, %r9d
	imull	$-1727483681, %ecx, %ecx
	xorl	2476(%rsi), %ecx
	orl	%r8d, %edi
	movl	896(%rsi), %r8d
	shrl	%edi
	xorl	%edi, %ecx
	movl	%r8d, %edi
	movl	%ecx, 888(%rsi)
	movl	%r8d, %ecx
	andl	$2147483647, %edi
	andl	$1, %ecx
	orl	%r9d, %edi
	movl	900(%rsi), %r9d
	imull	$-1727483681, %ecx, %ecx
	xorl	2480(%rsi), %ecx
	shrl	%edi
	andl	$-2147483648, %r8d
	xorl	%edi, %ecx
	movl	%r9d, %edi
	movl	%ecx, 892(%rsi)
	movl	%r9d, %ecx
	andl	$2147483647, %edi
	andl	$1, %ecx
	orl	%r8d, %edi
	movl	904(%rsi), %r8d
	imull	$-1727483681, %ecx, %ecx
	xorl	2484(%rsi), %ecx
	shrl	%edi
	andl	$-2147483648, %r9d
	xorl	%edi, %ecx
	movl	%r8d, %edi
	movl	%ecx, 896(%rsi)
	movl	%r8d, %ecx
	andl	$2147483647, %edi
	andl	$1, %ecx
	orl	%r9d, %edi
	movl	908(%rsi), %r9d
	imull	$-1727483681, %ecx, %ecx
	xorl	2488(%rsi), %ecx
	shrl	%edi
	xorl	%edi, %ecx
	movl	%r9d, %edi
	movl	%ecx, 900(%rsi)
	movl	%r9d, %ecx
	andl	$1, %ecx
	imull	$-1727483681, %ecx, %ecx
	xorl	2492(%rsi), %ecx
	andl	$2147483647, %edi
	andl	$-2147483648, %r8d
	orl	%r8d, %edi
	shrl	%edi
	xorl	%edi, %ecx
	testq	%r13, %r13
	movl	%ecx, 904(%rsi)
	je	.L63
	movl	912(%rsi), %edi
	andl	$-2147483648, %r9d
	movl	%edi, %ecx
	movl	%edi, %r8d
	andl	$1, %ecx
	andl	$2147483647, %r8d
	imull	$-1727483681, %ecx, %ecx
	xorl	(%rsi), %ecx
	orl	%r8d, %r9d
	shrl	%r9d
	xorl	%r9d, %ecx
	cmpq	$1, %r13
	movl	%ecx, 908(%rsi)
	je	.L64
	movl	916(%rsi), %r8d
	andl	$-2147483648, %edi
	movl	%r8d, %ecx
	movl	%r8d, %r9d
	andl	$1, %ecx
	andl	$2147483647, %r9d
	imull	$-1727483681, %ecx, %ecx
	xorl	4(%rsi), %ecx
	orl	%r9d, %edi
	shrl	%edi
	xorl	%edi, %ecx
	cmpq	$3, %r13
	movl	%ecx, 912(%rsi)
	jne	.L65
	movl	920(%rsi), %edi
	andl	$-2147483648, %r8d
	movl	$393, %r12d
	movl	$230, %ebp
	movl	%edi, %ecx
	andl	$2147483647, %edi
	andl	$1, %ecx
	orl	%edi, %r8d
	imull	$-1727483681, %ecx, %ecx
	xorl	8(%rsi), %ecx
	shrl	%r8d
	xorl	%r8d, %ecx
	movl	%ecx, 916(%rsi)
.L38:
	movq	64(%rsp), %rcx
	movq	88(%rsp), %r8
	xorl	%r10d, %r10d
	movq	32(%rsp), %rdi
	movq	%rcx, %r9
	jmp	.L44
	.p2align 5,,7
	.p2align 3
.L40:
	movq	%r11, %r10
.L44:
	vmovdqu	(%rdi), %xmm4
	vmovdqu	(%r8), %xmm7
	prefetcht0	176(%rcx)
	prefetcht0	176(%rdi)
	prefetcht0	176(%r8)
	leaq	4(%r10), %r11
	vpand	%xmm3, %xmm4, %xmm8
	vpand	%xmm1, %xmm4, %xmm4
	addq	$5, %r10
	prefetchw	176(%r9)
	addq	$64, %rcx
	addq	$64, %rdi
	vpmulld	%xmm2, %xmm8, %xmm8
	vpxor	%xmm7, %xmm8, %xmm7
	vpand	-64(%rcx), %xmm0, %xmm8
	addq	$64, %r8
	addq	$64, %r9
	vpor	%xmm4, %xmm8, %xmm4
	vpsrld	$1, %xmm4, %xmm4
	vpxor	%xmm4, %xmm7, %xmm4
	vmovaps	%xmm4, -64(%r9)
	vmovdqu	-48(%rdi), %xmm4
	vmovdqu	-48(%r8), %xmm7
	vpand	%xmm3, %xmm4, %xmm8
	vpand	%xmm1, %xmm4, %xmm4
	vpmulld	%xmm2, %xmm8, %xmm8
	vpxor	%xmm7, %xmm8, %xmm7
	vpand	-48(%rcx), %xmm0, %xmm8
	vpor	%xmm4, %xmm8, %xmm4
	vpsrld	$1, %xmm4, %xmm4
	vpxor	%xmm4, %xmm7, %xmm4
	vmovaps	%xmm4, -48(%r9)
	vmovdqu	-32(%rdi), %xmm4
	vmovdqu	-32(%r8), %xmm7
	vpand	%xmm3, %xmm4, %xmm8
	vpand	%xmm1, %xmm4, %xmm4
	vpmulld	%xmm2, %xmm8, %xmm8
	vpxor	%xmm7, %xmm8, %xmm7
	vpand	-32(%rcx), %xmm0, %xmm8
	vpor	%xmm4, %xmm8, %xmm4
	vpsrld	$1, %xmm4, %xmm4
	vpxor	%xmm4, %xmm7, %xmm4
	vmovaps	%xmm4, -32(%r9)
	vmovdqu	-16(%rdi), %xmm4
	vmovdqu	-16(%r8), %xmm7
	vpand	%xmm3, %xmm4, %xmm8
	vpand	%xmm1, %xmm4, %xmm4
	vpmulld	%xmm2, %xmm8, %xmm8
	vpxor	%xmm7, %xmm8, %xmm7
	vpand	-16(%rcx), %xmm0, %xmm8
	vpor	%xmm4, %xmm8, %xmm4
	vpsrld	$1, %xmm4, %xmm4
	vpxor	%xmm4, %xmm7, %xmm4
	vmovaps	%xmm4, -16(%r9)
	cmpq	%rbx, %r10
	jb	.L40
	xorl	%r10d, %r10d
	.p2align 5,,24
	.p2align 3
.L46:
	vmovdqu	(%rdi,%r10), %xmm4
	vmovdqu	(%r8,%r10), %xmm8
	incq	%r11
	vpand	%xmm3, %xmm4, %xmm7
	vpand	%xmm1, %xmm4, %xmm4
	vpmulld	%xmm2, %xmm7, %xmm7
	vpxor	%xmm8, %xmm7, %xmm7
	vpand	(%rcx,%r10), %xmm0, %xmm8
	vpor	%xmm8, %xmm4, %xmm4
	vpsrld	$1, %xmm4, %xmm4
	vpxor	%xmm4, %xmm7, %xmm4
	vmovaps	%xmm4, (%r9,%r10)
	addq	$16, %r10
	cmpq	%r11, %rax
	ja	.L46
	movq	72(%rsp), %rdi
	addq	%rdi, %rbp
	subq	%rdi, %r12
	cmpq	%rdi, 16(%rsp)
	je	.L42
	leaq	1(%rbp), %r9
	leaq	(%rsi,%rbp,4), %r10
	movl	(%rsi,%r9,4), %r8d
	movl	(%r10), %ecx
	movl	%r8d, %edi
	movl	%r8d, %r11d
	andl	$-2147483648, %ecx
	andl	$1, %edi
	andl	$2147483647, %r11d
	imull	$-1727483681, %edi, %edi
	xorl	-908(%rsi,%rbp,4), %edi
	orl	%r11d, %ecx
	shrl	%ecx
	xorl	%ecx, %edi
	cmpq	$1, %r12
	movl	%edi, (%r10)
	je	.L42
	leaq	2(%rbp), %r11
	andl	$-2147483648, %r8d
	movl	(%rsi,%r11,4), %r10d
	movl	%r10d, %ecx
	movl	%r10d, %edi
	andl	$1, %ecx
	andl	$2147483647, %edi
	imull	$-1727483681, %ecx, %ecx
	xorl	-904(%rsi,%rbp,4), %ecx
	orl	%r8d, %edi
	shrl	%edi
	xorl	%edi, %ecx
	cmpq	$2, %r12
	movl	%ecx, (%rsi,%r9,4)
	je	.L42
	movl	12(%rsi,%rbp,4), %edi
	andl	$-2147483648, %r10d
	movl	%edi, %ecx
	andl	$2147483647, %edi
	andl	$1, %ecx
	orl	%r10d, %edi
	imull	$-1727483681, %ecx, %ecx
	xorl	-900(%rsi,%rbp,4), %ecx
	shrl	%edi
	xorl	%edi, %ecx
	movl	%ecx, (%rsi,%r11,4)
.L42:
	movl	(%rsi), %ecx
	movl	2492(%rsi), %r9d
	movl	%ecx, %edi
	movl	%ecx, %r8d
	andl	$-2147483648, %r9d
	andl	$1, %edi
	andl	$2147483647, %r8d
	imull	$-1727483681, %edi, %edi
	xorl	1584(%rsi), %edi
	orl	%r9d, %r8d
	shrl	%r8d
	xorl	%r8d, %edi
	movl	$1, %r8d
	movl	%edi, 2492(%rsi)
	jmp	.L28
	.p2align 5,,7
	.p2align 3
.L86:
	movq	96(%rsp), %rbx
	vsubsd	%xmm4, %xmm5, %xmm0
	vmovsd	%xmm4, 24(%rbx)
	call	log
	vmulsd	.LC6(%rip), %xmm0, %xmm0
	vsqrtsd	%xmm0, %xmm1, %xmm1
	vucomisd	%xmm1, %xmm1
	jp	.L88
.L51:
	vmovsd	.LC7(%rip), %xmm0
	vmovsd	%xmm1, 32(%rbx)
	movb	$1, 40(%rbx)
	vmovsd	%xmm1, 8(%rsp)
	vmulsd	16(%rbx), %xmm0, %xmm0
	call	cos
	vmovsd	8(%rsp), %xmm1
	jmp	.L53
	.p2align 5,,7
	.p2align 3
.L60:
	movl	$222, %r12d
	xorl	%ebp, %ebp
	jmp	.L29
	.p2align 5,,7
	.p2align 3
.L54:
	movl	$222, %ebp
	xorl	%r12d, %r12d
	jmp	.L5
	.p2align 5,,7
	.p2align 3
.L57:
	movl	$396, %ebp
	movl	$227, %r12d
	jmp	.L14
	.p2align 5,,7
	.p2align 3
.L63:
	movl	$396, %r12d
	movl	$227, %ebp
	jmp	.L38
	.p2align 5,,7
	.p2align 3
.L64:
	movl	$395, %r12d
	movl	$228, %ebp
	jmp	.L38
	.p2align 5,,7
	.p2align 3
.L59:
	movl	$394, %ebp
	movl	$229, %r12d
	jmp	.L14
	.p2align 5,,7
	.p2align 3
.L58:
	movl	$395, %ebp
	movl	$228, %r12d
	jmp	.L14
	.p2align 5,,7
	.p2align 3
.L55:
	movl	$221, %ebp
	movl	$1, %r12d
	jmp	.L5
	.p2align 5,,7
	.p2align 3
.L65:
	movl	$394, %r12d
	movl	$229, %ebp
	jmp	.L38
	.p2align 5,,7
	.p2align 3
.L62:
	movl	$220, %r12d
	movl	$2, %ebp
	jmp	.L29
	.p2align 5,,7
	.p2align 3
.L56:
	movl	$220, %ebp
	movl	$2, %r12d
	jmp	.L5
	.p2align 5,,7
	.p2align 3
.L61:
	movl	$221, %r12d
	movl	$1, %ebp
	jmp	.L29
.L88:
	call	sqrt
	vmovapd	%xmm0, %xmm1
	jmp	.L51
	.cfi_endproc
.LFE9145:
	.size	_ZN5boost6random19normal_distributionIdEclINS0_23mersenne_twister_engineIjLm32ELm624ELm397ELm31ELj2567483615ELm11ELj4294967295ELm7ELj2636928640ELm15ELj4022730752ELm18ELj1812433253EEEEEdRT_, .-_ZN5boost6random19normal_distributionIdEclINS0_23mersenne_twister_engineIjLm32ELm624ELm397ELm31ELj2567483615ELm11ELj4294967295ELm7ELj2636928640ELm15ELj4022730752ELm18ELj1812433253EEEEEdRT_
	.section	.text._ZNSt6vectorIdSaIdEE19_M_emplace_back_auxIIdEEEvDpOT_,"axG",@progbits,_ZNSt6vectorIdSaIdEE19_M_emplace_back_auxIIdEEEvDpOT_,comdat
	.align 2
	.p2align 5,,31
	.weak	_ZNSt6vectorIdSaIdEE19_M_emplace_back_auxIIdEEEvDpOT_
	.type	_ZNSt6vectorIdSaIdEE19_M_emplace_back_auxIIdEEEvDpOT_, @function
_ZNSt6vectorIdSaIdEE19_M_emplace_back_auxIIdEEEvDpOT_:
.LFB9383:
	.cfi_startproc
	pushq	%r13
	.cfi_def_cfa_offset 16
	.cfi_offset 13, -16
	pushq	%r12
	.cfi_def_cfa_offset 24
	.cfi_offset 12, -24
	movl	$8, %r12d
	pushq	%rbp
	.cfi_def_cfa_offset 32
	.cfi_offset 6, -32
	pushq	%rbx
	.cfi_def_cfa_offset 40
	.cfi_offset 3, -40
	movq	%rdi, %rbx
	subq	$24, %rsp
	.cfi_def_cfa_offset 64
	movq	8(%rdi), %rdx
	subq	(%rdi), %rdx
	sarq	$3, %rdx
	jne	.L108
.L91:
	movq	%r12, %rdi
	movq	%rsi, 8(%rsp)
	call	_Znwm
	movq	(%rbx), %rcx
	movq	%rax, %rbp
	movq	8(%rbx), %rax
	movq	8(%rsp), %rsi
	movq	%rbp, %r13
	subq	%rcx, %rax
	vmovsd	(%rsi), %xmm0
	sarq	$3, %rax
	leaq	0(,%rax,8), %rdx
	addq	%rdx, %r13
	je	.L92
	vmovsd	%xmm0, 0(%r13)
.L92:
	testq	%rax, %rax
	movq	%rcx, %rdi
	je	.L93
	movq	%rbp, %rdi
	movq	%rcx, %rsi
	call	memmove
	movq	(%rbx), %rdi
.L93:
	addq	$8, %r13
	testq	%rdi, %rdi
	je	.L94
	call	_ZdlPv
.L94:
	movq	%rbp, (%rbx)
	addq	%r12, %rbp
	movq	%r13, 8(%rbx)
	movq	%rbp, 16(%rbx)
	addq	$24, %rsp
	.cfi_remember_state
	.cfi_def_cfa_offset 40
	popq	%rbx
	.cfi_def_cfa_offset 32
	popq	%rbp
	.cfi_def_cfa_offset 24
	popq	%r12
	.cfi_def_cfa_offset 16
	popq	%r13
	.cfi_def_cfa_offset 8
	ret
	.p2align 5,,7
	.p2align 3
.L108:
	.cfi_restore_state
	leaq	(%rdx,%rdx), %rax
	cmpq	%rax, %rdx
	jbe	.L109
	movq	$-8, %r12
	jmp	.L91
.L109:
	movabsq	$2305843009213693951, %rcx
	salq	$4, %rdx
	movq	$-8, %r12
	cmpq	%rcx, %rax
	cmovbe	%rdx, %r12
	jmp	.L91
	.cfi_endproc
.LFE9383:
	.size	_ZNSt6vectorIdSaIdEE19_M_emplace_back_auxIIdEEEvDpOT_, .-_ZNSt6vectorIdSaIdEE19_M_emplace_back_auxIIdEEEvDpOT_
	.weak	_ZNSt6vectorIdSaIdEE19_M_emplace_back_auxIJdEEEvDpOT_
	.set	_ZNSt6vectorIdSaIdEE19_M_emplace_back_auxIJdEEEvDpOT_,_ZNSt6vectorIdSaIdEE19_M_emplace_back_auxIIdEEEvDpOT_
	.section	.rodata.str1.1,"aMS",@progbits,1
.LC9:
	.string	"vector::reserve"
	.section	.text.startup,"ax",@progbits
	.p2align 5,,31
	.globl	main
	.type	main, @function
main:
.LFB8861:
	.cfi_startproc
	.cfi_personality 0x3,__gxx_personality_v0
	.cfi_lsda 0x3,.LLSDA8861
	pushq	%rbp
	.cfi_def_cfa_offset 16
	.cfi_offset 6, -16
	movl	$42, %ecx
	movl	$1, %edx
	movl	$608, %r8d
	movq	%rsp, %rbp
	.cfi_def_cfa_register 6
	pushq	%r15
	pushq	%r14
	pushq	%r13
	pushq	%r12
	pushq	%rbx
	andq	$-32, %rsp
	subq	$2800, %rsp
	.cfi_offset 15, -24
	.cfi_offset 14, -32
	.cfi_offset 13, -40
	.cfi_offset 12, -48
	.cfi_offset 3, -56
	leaq	272(%rsp), %rdi
	movl	$42, 272(%rsp)
	movq	%rdi, %rax
	.p2align 5,,24
	.p2align 3
.L112:
	movl	%ecx, %esi
	movq	%r8, %rbx
	prefetchw	84(%rax)
	shrl	$30, %esi
	xorl	%ecx, %esi
	imull	$1812433253, %esi, %esi
	addl	%edx, %esi
	movl	%esi, %ecx
	movl	%esi, 4(%rax)
	shrl	$30, %ecx
	xorl	%esi, %ecx
	imull	$1812433253, %ecx, %ecx
	leal	1(%rcx,%rdx), %esi
	movl	%esi, %ecx
	movl	%esi, 8(%rax)
	shrl	$30, %ecx
	xorl	%esi, %ecx
	imull	$1812433253, %ecx, %ecx
	leal	2(%rcx,%rdx), %esi
	movl	%esi, %ecx
	movl	%esi, 12(%rax)
	shrl	$30, %ecx
	xorl	%esi, %ecx
	imull	$1812433253, %ecx, %ecx
	leal	3(%rcx,%rdx), %esi
	movl	%esi, %ecx
	movl	%esi, 16(%rax)
	shrl	$30, %ecx
	xorl	%esi, %ecx
	imull	$1812433253, %ecx, %ecx
	leal	4(%rcx,%rdx), %esi
	movl	%esi, %ecx
	movl	%esi, 20(%rax)
	shrl	$30, %ecx
	xorl	%esi, %ecx
	imull	$1812433253, %ecx, %ecx
	leal	5(%rcx,%rdx), %esi
	movl	%esi, %ecx
	movl	%esi, 24(%rax)
	shrl	$30, %ecx
	xorl	%esi, %ecx
	imull	$1812433253, %ecx, %ecx
	leal	6(%rcx,%rdx), %esi
	movl	%esi, %ecx
	movl	%esi, 28(%rax)
	shrl	$30, %ecx
	xorl	%esi, %ecx
	imull	$1812433253, %ecx, %ecx
	leal	7(%rcx,%rdx), %esi
	movl	%esi, %ecx
	movl	%esi, 32(%rax)
	shrl	$30, %ecx
	xorl	%esi, %ecx
	imull	$1812433253, %ecx, %ecx
	leal	8(%rcx,%rdx), %esi
	movl	%esi, %ecx
	movl	%esi, 36(%rax)
	shrl	$30, %ecx
	xorl	%esi, %ecx
	imull	$1812433253, %ecx, %ecx
	leal	9(%rcx,%rdx), %esi
	movl	%esi, %ecx
	movl	%esi, 40(%rax)
	shrl	$30, %ecx
	xorl	%esi, %ecx
	imull	$1812433253, %ecx, %ecx
	subq	%rdx, %rbx
	addq	$64, %rax
	leal	10(%rcx,%rdx), %esi
	movl	%esi, %ecx
	movl	%esi, -20(%rax)
	shrl	$30, %ecx
	xorl	%esi, %ecx
	imull	$1812433253, %ecx, %ecx
	leal	11(%rcx,%rdx), %esi
	movl	%esi, %ecx
	movl	%esi, -16(%rax)
	shrl	$30, %ecx
	xorl	%esi, %ecx
	imull	$1812433253, %ecx, %ecx
	leal	12(%rcx,%rdx), %esi
	movl	%esi, %ecx
	movl	%esi, -12(%rax)
	shrl	$30, %ecx
	xorl	%esi, %ecx
	imull	$1812433253, %ecx, %ecx
	leal	13(%rcx,%rdx), %esi
	movl	%esi, %ecx
	movl	%esi, -8(%rax)
	shrl	$30, %ecx
	xorl	%esi, %ecx
	imull	$1812433253, %ecx, %ecx
	leal	14(%rcx,%rdx), %esi
	movl	%esi, %ecx
	movl	%esi, -4(%rax)
	shrl	$30, %ecx
	xorl	%esi, %ecx
	leaq	16(%rdx), %rsi
	imull	$1812433253, %ecx, %ecx
	leal	15(%rcx,%rdx), %ecx
	movl	%ecx, (%rax)
	cmpq	$609, %rsi
	je	.L111
	movq	%rsi, %rdx
	jmp	.L112
.L111:
	leaq	609(%rbx), %rsi
	movl	$609, %eax
	.p2align 5,,24
	.p2align 3
.L114:
	movl	%ecx, %edx
	shrl	$30, %edx
	xorl	%ecx, %edx
	imull	$1812433253, %edx, %edx
	leal	(%rdx,%rax), %ecx
	movl	%ecx, (%rdi,%rax,4)
	incq	%rax
	cmpq	%rsi, %rax
	jne	.L114
	vmovsd	.LC5(%rip), %xmm7
	movq	%rdi, 208(%rsp)
	movl	$800000, %edi
	movq	$624, 2768(%rsp)
	movq	$0, 216(%rsp)
	vmovsd	%xmm7, 224(%rsp)
	movq	$0, 232(%rsp)
	movq	$0, 240(%rsp)
	movq	$0, 248(%rsp)
	movb	$0, 256(%rsp)
.LEHB0:
	call	_Znwm
.LEHE0:
	movl	$800000, %edx
	movq	%rax, %rdi
	xorl	%esi, %esi
	movq	%rax, (%rsp)
	call	memset
	movl	$800000, %edi
	movq	$0, 176(%rsp)
	movq	$0, 184(%rsp)
	movq	$0, 192(%rsp)
.LEHB1:
	call	_Znwm
	movq	176(%rsp), %rdi
	movq	%rax, %rbx
	testq	%rdi, %rdi
	je	.L117
	call	_ZdlPv
.L117:
	movq	208(%rsp), %rsi
	movq	%rbx, 176(%rsp)
	leaq	216(%rsp), %rdi
	movq	%rbx, 184(%rsp)
	addq	$800000, %rbx
	movq	%rbx, 192(%rsp)
	call	_ZN5boost6random19normal_distributionIdEclINS0_23mersenne_twister_engineIjLm32ELm624ELm397ELm31ELj2567483615ELm11ELj4294967295ELm7ELj2636928640ELm15ELj4022730752ELm18ELj1812433253EEEEEdRT_
	vmulsd	.LC10(%rip), %xmm0, %xmm6
	movq	(%rsp), %rax
	movq	%rax, 112(%rsp)
	addq	$800000, %rax
	movq	%rax, 8(%rsp)
	vmovsd	%xmm6, 16(%rsp)
	.p2align 5,,24
	.p2align 3
.L174:
	movq	112(%rsp), %rax
	cmpb	$0, 256(%rsp)
	vmovsd	(%rax), %xmm7
	movq	208(%rsp), %rax
	vmovsd	%xmm7, 88(%rsp)
	jne	.L118
	movq	%rax, %r14
	movq	$222, 96(%rsp)
	leaq	908(%rax), %r13
	andl	$15, %r14d
	andl	$15, %r13d
	movq	$396, 104(%rsp)
	shrq	$2, %r14
	shrq	$2, %r13
	movq	2496(%rax), %r8
	negq	%r14
	negq	%r13
	vmovsd	.LC4(%rip), %xmm7
	andl	$3, %r14d
	subq	%r14, 96(%rsp)
	andl	$3, %r13d
	movq	96(%rsp), %rcx
	leaq	0(,%r14,4), %rdx
	subq	%r13, 104(%rsp)
	leaq	908(,%r13,4), %rsi
	vmovsd	.LC5(%rip), %xmm6
	vmovdqa	.LC2(%rip), %xmm1
	leaq	-908(%rsi), %r15
	vmovdqa	.LC3(%rip), %xmm0
	shrq	$2, %rcx
	leaq	0(,%rcx,4), %rbx
	movq	%rbx, 32(%rsp)
	leaq	(%rax,%rdx), %rbx
	movq	%rbx, 72(%rsp)
	leaq	4(%rax,%rdx), %rbx
	movq	%rbx, 64(%rsp)
	leaq	1588(%rax,%rdx), %rbx
	movq	104(%rsp), %rdx
	movq	%rbx, 56(%rsp)
	leaq	-3(%rcx), %rbx
	movq	%rbx, 120(%rsp)
	shrq	$2, %rdx
	leaq	0(,%rdx,4), %rbx
	movq	%rbx, 40(%rsp)
	leaq	(%rax,%rsi), %rbx
	movq	%rbx, 48(%rsp)
	leaq	4(%rax,%rsi), %rbx
	movq	%rbx, 80(%rsp)
	leaq	(%rax,%r15), %rbx
	movq	%rbx, 24(%rsp)
	leaq	-3(%rdx), %rbx
	movq	%rbx, 128(%rsp)
	jmp	.L142
	.p2align 5,,7
	.p2align 3
.L233:
	movl	(%rax,%r8,4), %esi
	incq	%r8
.L120:
	movl	%esi, %edi
	movq	%r8, 2496(%rax)
	shrl	$11, %edi
	xorl	%esi, %edi
	movl	%edi, %esi
	sall	$7, %esi
	andl	$-1658038656, %esi
	xorl	%edi, %esi
	movl	%esi, %edi
	sall	$15, %edi
	andl	$-272236544, %edi
	xorl	%esi, %edi
	movl	%edi, %esi
	shrl	$18, %esi
	xorl	%esi, %edi
	movq	%rdi, 136(%rsp)
	vcvtsi2sdq	136(%rsp), %xmm2, %xmm2
	vmulsd	%xmm7, %xmm2, %xmm2
	vucomisd	%xmm2, %xmm6
	ja	.L232
.L142:
	cmpq	$624, %r8
	jne	.L233
	testq	%r14, %r14
	je	.L180
	movl	4(%rax), %edi
	movl	(%rax), %esi
	movl	%edi, %r8d
	movl	%edi, %r9d
	andl	$-2147483648, %esi
	andl	$1, %r8d
	andl	$2147483647, %r9d
	imull	$-1727483681, %r8d, %r8d
	xorl	1588(%rax), %r8d
	orl	%r9d, %esi
	shrl	%esi
	xorl	%esi, %r8d
	cmpq	$1, %r14
	movl	%r8d, (%rax)
	je	.L181
	movl	8(%rax), %r8d
	andl	$-2147483648, %edi
	movl	%r8d, %esi
	movl	%r8d, %r9d
	andl	$1, %esi
	andl	$2147483647, %r9d
	imull	$-1727483681, %esi, %esi
	xorl	1592(%rax), %esi
	orl	%r9d, %edi
	shrl	%edi
	xorl	%edi, %esi
	cmpq	$3, %r14
	movl	%esi, 4(%rax)
	jne	.L182
	movl	12(%rax), %edi
	andl	$-2147483648, %r8d
	movl	$219, %ebx
	movl	$3, %r12d
	movl	%edi, %esi
	andl	$2147483647, %edi
	andl	$1, %esi
	orl	%edi, %r8d
	imull	$-1727483681, %esi, %esi
	xorl	1596(%rax), %esi
	shrl	%r8d
	xorl	%r8d, %esi
	movl	%esi, 8(%rax)
.L121:
	movq	72(%rsp), %rsi
	movq	56(%rsp), %r8
	xorl	%r10d, %r10d
	movq	64(%rsp), %rdi
	movq	%rsi, %r9
	jmp	.L127
	.p2align 5,,7
	.p2align 3
.L123:
	movq	%r11, %r10
.L127:
	vmovdqu	(%rdi), %xmm2
	vmovdqu	(%r8), %xmm3
	prefetcht0	176(%rsi)
	prefetcht0	176(%rdi)
	prefetcht0	176(%r8)
	prefetchw	176(%r9)
	vpand	.LC0(%rip), %xmm2, %xmm4
	vpand	%xmm1, %xmm2, %xmm2
	addq	$64, %rsi
	addq	$64, %rdi
	addq	$64, %r8
	addq	$64, %r9
	leaq	4(%r10), %r11
	addq	$5, %r10
	vpmulld	.LC1(%rip), %xmm4, %xmm4
	vpxor	%xmm3, %xmm4, %xmm3
	vpand	-64(%rsi), %xmm0, %xmm4
	vpor	%xmm4, %xmm2, %xmm2
	vpsrld	$1, %xmm2, %xmm2
	vpxor	%xmm2, %xmm3, %xmm2
	vmovaps	%xmm2, -64(%r9)
	vmovdqu	-48(%rdi), %xmm2
	vmovdqu	-48(%r8), %xmm3
	vpand	.LC0(%rip), %xmm2, %xmm4
	vpand	%xmm1, %xmm2, %xmm2
	vpmulld	.LC1(%rip), %xmm4, %xmm4
	vpxor	%xmm3, %xmm4, %xmm3
	vpand	-48(%rsi), %xmm0, %xmm4
	vpor	%xmm4, %xmm2, %xmm2
	vpsrld	$1, %xmm2, %xmm2
	vpxor	%xmm2, %xmm3, %xmm2
	vmovaps	%xmm2, -48(%r9)
	vmovdqu	-32(%rdi), %xmm2
	vmovdqu	-32(%r8), %xmm3
	vpand	.LC0(%rip), %xmm2, %xmm4
	vpand	%xmm1, %xmm2, %xmm2
	vpmulld	.LC1(%rip), %xmm4, %xmm4
	vpxor	%xmm3, %xmm4, %xmm3
	vpand	-32(%rsi), %xmm0, %xmm4
	vpor	%xmm4, %xmm2, %xmm2
	vpsrld	$1, %xmm2, %xmm2
	vpxor	%xmm2, %xmm3, %xmm2
	vmovaps	%xmm2, -32(%r9)
	vmovdqu	-16(%rdi), %xmm2
	vmovdqu	-16(%r8), %xmm3
	vpand	.LC0(%rip), %xmm2, %xmm4
	vpand	%xmm1, %xmm2, %xmm2
	vpmulld	.LC1(%rip), %xmm4, %xmm4
	vpxor	%xmm3, %xmm4, %xmm3
	vpand	-16(%rsi), %xmm0, %xmm4
	vpor	%xmm4, %xmm2, %xmm2
	vpsrld	$1, %xmm2, %xmm2
	vpxor	%xmm2, %xmm3, %xmm2
	vmovaps	%xmm2, -16(%r9)
	cmpq	120(%rsp), %r10
	jb	.L123
	xorl	%r10d, %r10d
	.p2align 5,,24
	.p2align 3
.L129:
	vmovdqu	(%rdi,%r10), %xmm2
	vmovdqu	(%r8,%r10), %xmm4
	incq	%r11
	vpand	.LC0(%rip), %xmm2, %xmm3
	vpand	%xmm1, %xmm2, %xmm2
	vpmulld	.LC1(%rip), %xmm3, %xmm3
	vpxor	%xmm4, %xmm3, %xmm3
	vpand	(%rsi,%r10), %xmm0, %xmm4
	vpor	%xmm4, %xmm2, %xmm2
	vpsrld	$1, %xmm2, %xmm2
	vpxor	%xmm2, %xmm3, %xmm2
	vmovaps	%xmm2, (%r9,%r10)
	addq	$16, %r10
	cmpq	%rcx, %r11
	jb	.L129
	movq	32(%rsp), %rdi
	addq	%rdi, %r12
	subq	%rdi, %rbx
	cmpq	%rdi, 96(%rsp)
	je	.L125
	leaq	1(%r12), %r9
	leaq	(%rax,%r12,4), %r11
	movl	(%rax,%r9,4), %r8d
	movl	(%r11), %r10d
	movl	%r8d, %esi
	movl	%r8d, %edi
	andl	$-2147483648, %r10d
	andl	$1, %esi
	andl	$2147483647, %edi
	imull	$-1727483681, %esi, %esi
	xorl	1588(%rax,%r12,4), %esi
	orl	%r10d, %edi
	shrl	%edi
	xorl	%edi, %esi
	cmpq	$1, %rbx
	movl	%esi, (%r11)
	je	.L125
	leaq	2(%r12), %r11
	andl	$-2147483648, %r8d
	movl	(%rax,%r11,4), %r10d
	movl	%r10d, %esi
	movl	%r10d, %edi
	andl	$1, %esi
	andl	$2147483647, %edi
	imull	$-1727483681, %esi, %esi
	xorl	1592(%rax,%r12,4), %esi
	orl	%r8d, %edi
	shrl	%edi
	xorl	%edi, %esi
	cmpq	$2, %rbx
	movl	%esi, (%rax,%r9,4)
	je	.L125
	movl	12(%rax,%r12,4), %edi
	andl	$-2147483648, %r10d
	movl	%edi, %esi
	andl	$2147483647, %edi
	andl	$1, %esi
	orl	%r10d, %edi
	imull	$-1727483681, %esi, %esi
	xorl	1596(%rax,%r12,4), %esi
	shrl	%edi
	xorl	%edi, %esi
	movl	%esi, (%rax,%r11,4)
.L125:
	movl	892(%rax), %r9d
	movl	888(%rax), %r8d
	movl	%r9d, %esi
	movl	%r9d, %edi
	andl	$-2147483648, %r8d
	andl	$1, %esi
	andl	$2147483647, %edi
	andl	$-2147483648, %r9d
	imull	$-1727483681, %esi, %esi
	xorl	2476(%rax), %esi
	orl	%r8d, %edi
	movl	896(%rax), %r8d
	shrl	%edi
	xorl	%edi, %esi
	movl	%r8d, %edi
	movl	%esi, 888(%rax)
	movl	%r8d, %esi
	andl	$2147483647, %edi
	andl	$1, %esi
	orl	%r9d, %edi
	movl	900(%rax), %r9d
	imull	$-1727483681, %esi, %esi
	xorl	2480(%rax), %esi
	shrl	%edi
	andl	$-2147483648, %r8d
	xorl	%edi, %esi
	movl	%r9d, %edi
	movl	%esi, 892(%rax)
	movl	%r9d, %esi
	andl	$2147483647, %edi
	andl	$1, %esi
	orl	%r8d, %edi
	movl	904(%rax), %r8d
	imull	$-1727483681, %esi, %esi
	xorl	2484(%rax), %esi
	shrl	%edi
	andl	$-2147483648, %r9d
	xorl	%edi, %esi
	movl	%r8d, %edi
	movl	%esi, 896(%rax)
	movl	%r8d, %esi
	andl	$2147483647, %edi
	andl	$1, %esi
	orl	%r9d, %edi
	movl	908(%rax), %r9d
	imull	$-1727483681, %esi, %esi
	xorl	2488(%rax), %esi
	shrl	%edi
	xorl	%edi, %esi
	movl	%r9d, %edi
	movl	%esi, 900(%rax)
	movl	%r9d, %esi
	andl	$1, %esi
	imull	$-1727483681, %esi, %esi
	xorl	2492(%rax), %esi
	andl	$2147483647, %edi
	andl	$-2147483648, %r8d
	orl	%r8d, %edi
	shrl	%edi
	xorl	%edi, %esi
	testq	%r13, %r13
	movl	%esi, 904(%rax)
	je	.L183
	movl	912(%rax), %edi
	andl	$-2147483648, %r9d
	movl	%edi, %esi
	movl	%edi, %r8d
	andl	$1, %esi
	andl	$2147483647, %r8d
	imull	$-1727483681, %esi, %esi
	xorl	(%rax), %esi
	orl	%r8d, %r9d
	shrl	%r9d
	xorl	%r9d, %esi
	cmpq	$1, %r13
	movl	%esi, 908(%rax)
	je	.L184
	movl	916(%rax), %r8d
	andl	$-2147483648, %edi
	movl	%r8d, %esi
	movl	%r8d, %r9d
	andl	$1, %esi
	andl	$2147483647, %r9d
	imull	$-1727483681, %esi, %esi
	xorl	4(%rax), %esi
	orl	%r9d, %edi
	shrl	%edi
	xorl	%edi, %esi
	cmpq	$3, %r13
	movl	%esi, 912(%rax)
	jne	.L185
	movl	920(%rax), %edi
	andl	$-2147483648, %r8d
	movl	$393, %ebx
	movl	$230, %r12d
	movl	%edi, %esi
	andl	$2147483647, %edi
	andl	$1, %esi
	orl	%edi, %r8d
	imull	$-1727483681, %esi, %esi
	xorl	8(%rax), %esi
	shrl	%r8d
	xorl	%r8d, %esi
	movl	%esi, 916(%rax)
.L130:
	movq	48(%rsp), %rsi
	movq	24(%rsp), %r8
	xorl	%r10d, %r10d
	movq	80(%rsp), %rdi
	movq	%rsi, %r9
	jmp	.L136
	.p2align 5,,7
	.p2align 3
.L132:
	movq	%r11, %r10
.L136:
	vmovdqu	(%rdi), %xmm2
	vmovdqu	(%r8), %xmm3
	prefetcht0	176(%rsi)
	prefetcht0	176(%rdi)
	prefetcht0	176(%r8)
	prefetchw	176(%r9)
	vpand	.LC0(%rip), %xmm2, %xmm4
	vpand	%xmm1, %xmm2, %xmm2
	addq	$64, %rsi
	addq	$64, %rdi
	addq	$64, %r8
	addq	$64, %r9
	leaq	4(%r10), %r11
	addq	$5, %r10
	vpmulld	.LC1(%rip), %xmm4, %xmm4
	vpxor	%xmm3, %xmm4, %xmm3
	vpand	-64(%rsi), %xmm0, %xmm4
	vpor	%xmm4, %xmm2, %xmm2
	vpsrld	$1, %xmm2, %xmm2
	vpxor	%xmm2, %xmm3, %xmm2
	vmovaps	%xmm2, -64(%r9)
	vmovdqu	-48(%rdi), %xmm2
	vmovdqu	-48(%r8), %xmm3
	vpand	.LC0(%rip), %xmm2, %xmm4
	vpand	%xmm1, %xmm2, %xmm2
	vpmulld	.LC1(%rip), %xmm4, %xmm4
	vpxor	%xmm3, %xmm4, %xmm3
	vpand	-48(%rsi), %xmm0, %xmm4
	vpor	%xmm4, %xmm2, %xmm2
	vpsrld	$1, %xmm2, %xmm2
	vpxor	%xmm2, %xmm3, %xmm2
	vmovaps	%xmm2, -48(%r9)
	vmovdqu	-32(%rdi), %xmm2
	vmovdqu	-32(%r8), %xmm3
	vpand	.LC0(%rip), %xmm2, %xmm4
	vpand	%xmm1, %xmm2, %xmm2
	vpmulld	.LC1(%rip), %xmm4, %xmm4
	vpxor	%xmm3, %xmm4, %xmm3
	vpand	-32(%rsi), %xmm0, %xmm4
	vpor	%xmm4, %xmm2, %xmm2
	vpsrld	$1, %xmm2, %xmm2
	vpxor	%xmm2, %xmm3, %xmm2
	vmovaps	%xmm2, -32(%r9)
	vmovdqu	-16(%rdi), %xmm2
	vmovdqu	-16(%r8), %xmm3
	vpand	.LC0(%rip), %xmm2, %xmm4
	vpand	%xmm1, %xmm2, %xmm2
	vpmulld	.LC1(%rip), %xmm4, %xmm4
	vpxor	%xmm3, %xmm4, %xmm3
	vpand	-16(%rsi), %xmm0, %xmm4
	vpor	%xmm4, %xmm2, %xmm2
	vpsrld	$1, %xmm2, %xmm2
	vpxor	%xmm2, %xmm3, %xmm2
	vmovaps	%xmm2, -16(%r9)
	cmpq	128(%rsp), %r10
	jb	.L132
	xorl	%r10d, %r10d
	.p2align 5,,24
	.p2align 3
.L138:
	vmovdqu	(%rdi,%r10), %xmm2
	vmovdqu	(%r8,%r10), %xmm4
	incq	%r11
	vpand	.LC0(%rip), %xmm2, %xmm3
	vpand	%xmm1, %xmm2, %xmm2
	vpmulld	.LC1(%rip), %xmm3, %xmm3
	vpxor	%xmm4, %xmm3, %xmm3
	vpand	(%rsi,%r10), %xmm0, %xmm4
	vpor	%xmm4, %xmm2, %xmm2
	vpsrld	$1, %xmm2, %xmm2
	vpxor	%xmm2, %xmm3, %xmm2
	vmovaps	%xmm2, (%r9,%r10)
	addq	$16, %r10
	cmpq	%rdx, %r11
	jb	.L138
	movq	40(%rsp), %rdi
	addq	%rdi, %r12
	subq	%rdi, %rbx
	cmpq	%rdi, 104(%rsp)
	je	.L134
	leaq	1(%r12), %r9
	leaq	(%rax,%r12,4), %r11
	movl	(%rax,%r9,4), %r8d
	movl	(%r11), %r10d
	movl	%r8d, %esi
	movl	%r8d, %edi
	andl	$-2147483648, %r10d
	andl	$1, %esi
	andl	$2147483647, %edi
	imull	$-1727483681, %esi, %esi
	xorl	-908(%rax,%r12,4), %esi
	orl	%r10d, %edi
	shrl	%edi
	xorl	%edi, %esi
	cmpq	$1, %rbx
	movl	%esi, (%r11)
	je	.L134
	leaq	2(%r12), %r11
	andl	$-2147483648, %r8d
	movl	(%rax,%r11,4), %r10d
	movl	%r10d, %esi
	movl	%r10d, %edi
	andl	$1, %esi
	andl	$2147483647, %edi
	imull	$-1727483681, %esi, %esi
	xorl	-904(%rax,%r12,4), %esi
	orl	%r8d, %edi
	shrl	%edi
	xorl	%edi, %esi
	cmpq	$2, %rbx
	movl	%esi, (%rax,%r9,4)
	je	.L134
	movl	12(%rax,%r12,4), %edi
	andl	$-2147483648, %r10d
	movl	%edi, %esi
	andl	$2147483647, %edi
	andl	$1, %esi
	orl	%r10d, %edi
	imull	$-1727483681, %esi, %esi
	xorl	-900(%rax,%r12,4), %esi
	shrl	%edi
	xorl	%edi, %esi
	movl	%esi, (%rax,%r11,4)
.L134:
	movl	(%rax), %esi
	movl	2492(%rax), %r9d
	movl	%esi, %edi
	movl	%esi, %r8d
	andl	$-2147483648, %r9d
	andl	$1, %edi
	andl	$2147483647, %r8d
	imull	$-1727483681, %edi, %edi
	xorl	1584(%rax), %edi
	orl	%r9d, %r8d
	shrl	%r8d
	xorl	%r8d, %edi
	movl	$1, %r8d
	movl	%edi, 2492(%rax)
	jmp	.L120
	.p2align 5,,7
	.p2align 3
.L118:
	vmovsd	.LC7(%rip), %xmm6
	vmovsd	248(%rsp), %xmm1
	movb	$0, 256(%rsp)
	vmulsd	232(%rsp), %xmm6, %xmm0
	vmovsd	%xmm1, 128(%rsp)
	call	sin
	vmovsd	128(%rsp), %xmm1
.L169:
	vmulsd	%xmm0, %xmm1, %xmm0
	movq	184(%rsp), %rax
	cmpq	192(%rsp), %rax
	vmulsd	224(%rsp), %xmm0, %xmm0
	vaddsd	216(%rsp), %xmm0, %xmm0
	vmulsd	16(%rsp), %xmm0, %xmm0
	vaddsd	88(%rsp), %xmm0, %xmm0
	vmovsd	%xmm0, 144(%rsp)
	je	.L170
	testq	%rax, %rax
	je	.L171
	vmovsd	%xmm0, (%rax)
.L171:
	addq	$8, %rax
	movq	%rax, 184(%rsp)
.L172:
	addq	$8, 112(%rsp)
	movq	8(%rsp), %rax
	cmpq	%rax, 112(%rsp)
	jne	.L174
	movq	176(%rsp), %rdi
	testq	%rdi, %rdi
	je	.L175
	call	_ZdlPv
.L175:
	movq	(%rsp), %rdi
	testq	%rdi, %rdi
	je	.L230
	call	_ZdlPv
.L230:
	leaq	-40(%rbp), %rsp
	xorl	%eax, %eax
	popq	%rbx
	popq	%r12
	popq	%r13
	popq	%r14
	popq	%r15
	popq	%rbp
	.cfi_remember_state
	.cfi_def_cfa 7, 8
	ret
	.p2align 5,,7
	.p2align 3
.L232:
	.cfi_restore_state
	leaq	(%rax,%r15), %rbx
	vmovsd	%xmm2, 232(%rsp)
	vmovdqa	.LC2(%rip), %xmm1
	movq	%rbx, 24(%rsp)
	vmovdqa	.LC1(%rip), %xmm2
	vmovdqa	.LC3(%rip), %xmm0
	jmp	.L166
	.p2align 5,,7
	.p2align 3
.L235:
	movl	(%rax,%r8,4), %esi
	incq	%r8
.L144:
	movl	%esi, %edi
	movq	%r8, 2496(%rax)
	shrl	$11, %edi
	xorl	%esi, %edi
	movl	%edi, %esi
	sall	$7, %esi
	andl	$-1658038656, %esi
	xorl	%edi, %esi
	movl	%esi, %edi
	sall	$15, %edi
	andl	$-272236544, %edi
	xorl	%esi, %edi
	movl	%edi, %esi
	shrl	$18, %esi
	xorl	%esi, %edi
	movq	%rdi, 136(%rsp)
	vcvtsi2sdq	136(%rsp), %xmm3, %xmm3
	vmulsd	%xmm7, %xmm3, %xmm3
	vucomisd	%xmm3, %xmm6
	ja	.L234
.L166:
	cmpq	$624, %r8
	jne	.L235
	testq	%r14, %r14
	je	.L186
	movl	4(%rax), %edi
	movl	(%rax), %esi
	movl	%edi, %r8d
	movl	%edi, %r9d
	andl	$-2147483648, %esi
	andl	$1, %r8d
	andl	$2147483647, %r9d
	imull	$-1727483681, %r8d, %r8d
	xorl	1588(%rax), %r8d
	orl	%r9d, %esi
	shrl	%esi
	xorl	%esi, %r8d
	cmpq	$1, %r14
	movl	%r8d, (%rax)
	je	.L187
	movl	8(%rax), %r8d
	andl	$-2147483648, %edi
	movl	%r8d, %esi
	movl	%r8d, %r9d
	andl	$1, %esi
	andl	$2147483647, %r9d
	imull	$-1727483681, %esi, %esi
	xorl	1592(%rax), %esi
	orl	%r9d, %edi
	shrl	%edi
	xorl	%edi, %esi
	cmpq	$3, %r14
	movl	%esi, 4(%rax)
	jne	.L188
	movl	12(%rax), %edi
	andl	$-2147483648, %r8d
	movl	$219, %r12d
	movl	$3, %ebx
	movl	%edi, %esi
	andl	$2147483647, %edi
	andl	$1, %esi
	orl	%edi, %r8d
	imull	$-1727483681, %esi, %esi
	xorl	1596(%rax), %esi
	shrl	%r8d
	xorl	%r8d, %esi
	movl	%esi, 8(%rax)
.L145:
	movq	72(%rsp), %rsi
	movq	56(%rsp), %r8
	xorl	%r10d, %r10d
	movq	64(%rsp), %rdi
	movq	120(%rsp), %r15
	movq	%rsi, %r9
	jmp	.L151
	.p2align 5,,7
	.p2align 3
.L147:
	movq	%r11, %r10
.L151:
	vmovdqu	(%rdi), %xmm3
	vmovdqu	(%r8), %xmm4
	prefetcht0	176(%rsi)
	prefetcht0	176(%rdi)
	prefetcht0	176(%r8)
	leaq	4(%r10), %r11
	vpand	.LC0(%rip), %xmm3, %xmm5
	vpand	%xmm1, %xmm3, %xmm3
	addq	$5, %r10
	prefetchw	176(%r9)
	addq	$64, %rsi
	addq	$64, %rdi
	addq	$64, %r8
	addq	$64, %r9
	vpmulld	%xmm2, %xmm5, %xmm5
	vpxor	%xmm4, %xmm5, %xmm4
	vpand	-64(%rsi), %xmm0, %xmm5
	vpor	%xmm3, %xmm5, %xmm3
	vpsrld	$1, %xmm3, %xmm3
	vpxor	%xmm3, %xmm4, %xmm3
	vmovaps	%xmm3, -64(%r9)
	vmovdqu	-48(%rdi), %xmm3
	vmovdqu	-48(%r8), %xmm4
	vpand	.LC0(%rip), %xmm3, %xmm5
	vpand	%xmm1, %xmm3, %xmm3
	vpmulld	%xmm2, %xmm5, %xmm5
	vpxor	%xmm4, %xmm5, %xmm4
	vpand	-48(%rsi), %xmm0, %xmm5
	vpor	%xmm5, %xmm3, %xmm3
	vpsrld	$1, %xmm3, %xmm3
	vpxor	%xmm3, %xmm4, %xmm3
	vmovaps	%xmm3, -48(%r9)
	vmovdqu	-32(%rdi), %xmm3
	vmovdqu	-32(%r8), %xmm4
	vpand	.LC0(%rip), %xmm3, %xmm5
	vpand	%xmm1, %xmm3, %xmm3
	vpmulld	%xmm2, %xmm5, %xmm5
	vpxor	%xmm4, %xmm5, %xmm4
	vpand	-32(%rsi), %xmm0, %xmm5
	vpor	%xmm5, %xmm3, %xmm3
	vpsrld	$1, %xmm3, %xmm3
	vpxor	%xmm3, %xmm4, %xmm3
	vmovaps	%xmm3, -32(%r9)
	vmovdqu	-16(%rdi), %xmm3
	vmovdqu	-16(%r8), %xmm4
	vpand	.LC0(%rip), %xmm3, %xmm5
	vpand	%xmm1, %xmm3, %xmm3
	vpmulld	%xmm2, %xmm5, %xmm5
	vpxor	%xmm4, %xmm5, %xmm4
	vpand	-16(%rsi), %xmm0, %xmm5
	vpor	%xmm5, %xmm3, %xmm3
	vpsrld	$1, %xmm3, %xmm3
	vpxor	%xmm3, %xmm4, %xmm3
	vmovaps	%xmm3, -16(%r9)
	cmpq	%r15, %r10
	jb	.L147
	xorl	%r10d, %r10d
	.p2align 5,,24
	.p2align 3
.L153:
	vmovdqu	(%rdi,%r10), %xmm3
	vmovdqu	(%r8,%r10), %xmm5
	incq	%r11
	vpand	.LC0(%rip), %xmm3, %xmm4
	vpand	%xmm1, %xmm3, %xmm3
	vpmulld	%xmm2, %xmm4, %xmm4
	vpxor	%xmm5, %xmm4, %xmm4
	vpand	(%rsi,%r10), %xmm0, %xmm5
	vpor	%xmm3, %xmm5, %xmm3
	vpsrld	$1, %xmm3, %xmm3
	vpxor	%xmm3, %xmm4, %xmm3
	vmovaps	%xmm3, (%r9,%r10)
	addq	$16, %r10
	cmpq	%rcx, %r11
	jb	.L153
	movq	32(%rsp), %rdi
	addq	%rdi, %rbx
	subq	%rdi, %r12
	cmpq	%rdi, 96(%rsp)
	je	.L149
	leaq	1(%rbx), %r9
	leaq	(%rax,%rbx,4), %r11
	movl	(%rax,%r9,4), %r8d
	movl	(%r11), %r10d
	movl	%r8d, %esi
	movl	%r8d, %edi
	andl	$-2147483648, %r10d
	andl	$1, %esi
	andl	$2147483647, %edi
	imull	$-1727483681, %esi, %esi
	xorl	1588(%rax,%rbx,4), %esi
	orl	%r10d, %edi
	shrl	%edi
	xorl	%edi, %esi
	cmpq	$1, %r12
	movl	%esi, (%r11)
	je	.L149
	leaq	2(%rbx), %r11
	andl	$-2147483648, %r8d
	movl	(%rax,%r11,4), %r10d
	movl	%r10d, %esi
	movl	%r10d, %edi
	andl	$1, %esi
	andl	$2147483647, %edi
	imull	$-1727483681, %esi, %esi
	xorl	1592(%rax,%rbx,4), %esi
	orl	%r8d, %edi
	shrl	%edi
	xorl	%edi, %esi
	cmpq	$2, %r12
	movl	%esi, (%rax,%r9,4)
	je	.L149
	movl	12(%rax,%rbx,4), %edi
	andl	$-2147483648, %r10d
	movl	%edi, %esi
	andl	$2147483647, %edi
	andl	$1, %esi
	orl	%r10d, %edi
	imull	$-1727483681, %esi, %esi
	xorl	1596(%rax,%rbx,4), %esi
	shrl	%edi
	xorl	%edi, %esi
	movl	%esi, (%rax,%r11,4)
.L149:
	movl	892(%rax), %r9d
	movl	888(%rax), %r8d
	movl	%r9d, %esi
	movl	%r9d, %edi
	andl	$-2147483648, %r8d
	andl	$1, %esi
	andl	$2147483647, %edi
	andl	$-2147483648, %r9d
	imull	$-1727483681, %esi, %esi
	xorl	2476(%rax), %esi
	orl	%r8d, %edi
	movl	896(%rax), %r8d
	shrl	%edi
	xorl	%edi, %esi
	movl	%r8d, %edi
	movl	%esi, 888(%rax)
	movl	%r8d, %esi
	andl	$2147483647, %edi
	andl	$1, %esi
	orl	%r9d, %edi
	movl	900(%rax), %r9d
	imull	$-1727483681, %esi, %esi
	xorl	2480(%rax), %esi
	shrl	%edi
	andl	$-2147483648, %r8d
	xorl	%edi, %esi
	movl	%r9d, %edi
	movl	%esi, 892(%rax)
	movl	%r9d, %esi
	andl	$2147483647, %edi
	andl	$1, %esi
	orl	%r8d, %edi
	movl	904(%rax), %r8d
	imull	$-1727483681, %esi, %esi
	xorl	2484(%rax), %esi
	shrl	%edi
	andl	$-2147483648, %r9d
	xorl	%edi, %esi
	movl	%r8d, %edi
	movl	%esi, 896(%rax)
	movl	%r8d, %esi
	andl	$2147483647, %edi
	andl	$1, %esi
	orl	%r9d, %edi
	movl	908(%rax), %r9d
	imull	$-1727483681, %esi, %esi
	xorl	2488(%rax), %esi
	shrl	%edi
	xorl	%edi, %esi
	movl	%r9d, %edi
	movl	%esi, 900(%rax)
	movl	%r9d, %esi
	andl	$1, %esi
	imull	$-1727483681, %esi, %esi
	xorl	2492(%rax), %esi
	andl	$2147483647, %edi
	andl	$-2147483648, %r8d
	orl	%r8d, %edi
	shrl	%edi
	xorl	%edi, %esi
	testq	%r13, %r13
	movl	%esi, 904(%rax)
	je	.L189
	movl	912(%rax), %edi
	andl	$-2147483648, %r9d
	movl	%edi, %esi
	movl	%edi, %r8d
	andl	$1, %esi
	andl	$2147483647, %r8d
	imull	$-1727483681, %esi, %esi
	xorl	(%rax), %esi
	orl	%r8d, %r9d
	shrl	%r9d
	xorl	%r9d, %esi
	cmpq	$1, %r13
	movl	%esi, 908(%rax)
	je	.L190
	movl	916(%rax), %r8d
	andl	$-2147483648, %edi
	movl	%r8d, %esi
	movl	%r8d, %r9d
	andl	$1, %esi
	andl	$2147483647, %r9d
	imull	$-1727483681, %esi, %esi
	xorl	4(%rax), %esi
	orl	%r9d, %edi
	shrl	%edi
	xorl	%edi, %esi
	cmpq	$3, %r13
	movl	%esi, 912(%rax)
	jne	.L191
	movl	920(%rax), %edi
	andl	$-2147483648, %r8d
	movl	$393, %r12d
	movl	$230, %ebx
	movl	%edi, %esi
	andl	$2147483647, %edi
	andl	$1, %esi
	orl	%edi, %r8d
	imull	$-1727483681, %esi, %esi
	xorl	8(%rax), %esi
	shrl	%r8d
	xorl	%r8d, %esi
	movl	%esi, 916(%rax)
.L154:
	movq	48(%rsp), %rsi
	movq	24(%rsp), %r8
	movl	$4, %r10d
	movq	80(%rsp), %rdi
	movq	%rax, %r11
	movq	%rsi, %r9
	jmp	.L160
	.p2align 5,,7
	.p2align 3
.L156:
	movq	%rax, %r10
.L160:
	vmovdqu	(%rdi), %xmm3
	vmovdqu	(%r8), %xmm4
	prefetcht0	176(%rsi)
	prefetcht0	176(%rdi)
	prefetcht0	176(%r8)
	prefetchw	176(%r9)
	vpand	.LC0(%rip), %xmm3, %xmm5
	vpand	%xmm1, %xmm3, %xmm3
	addq	$64, %rsi
	addq	$64, %rdi
	addq	$64, %r8
	addq	$64, %r9
	leaq	1(%r10), %r15
	leaq	4(%r10), %rax
	vpmulld	%xmm2, %xmm5, %xmm5
	vpxor	%xmm4, %xmm5, %xmm4
	vpand	-64(%rsi), %xmm0, %xmm5
	vpor	%xmm3, %xmm5, %xmm3
	vpsrld	$1, %xmm3, %xmm3
	vpxor	%xmm3, %xmm4, %xmm3
	vmovaps	%xmm3, -64(%r9)
	vmovdqu	-48(%rdi), %xmm3
	vmovdqu	-48(%r8), %xmm4
	vpand	.LC0(%rip), %xmm3, %xmm5
	vpand	%xmm1, %xmm3, %xmm3
	vpmulld	%xmm2, %xmm5, %xmm5
	vpxor	%xmm4, %xmm5, %xmm4
	vpand	-48(%rsi), %xmm0, %xmm5
	vpor	%xmm3, %xmm5, %xmm3
	vpsrld	$1, %xmm3, %xmm3
	vpxor	%xmm3, %xmm4, %xmm3
	vmovaps	%xmm3, -48(%r9)
	vmovdqu	-32(%rdi), %xmm3
	vmovdqu	-32(%r8), %xmm4
	vpand	.LC0(%rip), %xmm3, %xmm5
	vpand	%xmm1, %xmm3, %xmm3
	vpmulld	%xmm2, %xmm5, %xmm5
	vpxor	%xmm4, %xmm5, %xmm4
	vpand	-32(%rsi), %xmm0, %xmm5
	vpor	%xmm3, %xmm5, %xmm3
	vpsrld	$1, %xmm3, %xmm3
	vpxor	%xmm3, %xmm4, %xmm3
	vmovaps	%xmm3, -32(%r9)
	vmovdqu	-16(%rdi), %xmm3
	vmovdqu	-16(%r8), %xmm4
	vpand	.LC0(%rip), %xmm3, %xmm5
	vpand	%xmm1, %xmm3, %xmm3
	vpmulld	%xmm2, %xmm5, %xmm5
	vpxor	%xmm4, %xmm5, %xmm4
	vpand	-16(%rsi), %xmm0, %xmm5
	vpor	%xmm3, %xmm5, %xmm3
	vpsrld	$1, %xmm3, %xmm3
	vpxor	%xmm3, %xmm4, %xmm3
	vmovaps	%xmm3, -16(%r9)
	cmpq	128(%rsp), %r15
	jb	.L156
	movq	%r11, %rax
	xorl	%r11d, %r11d
	.p2align 5,,24
	.p2align 3
.L162:
	vmovdqu	(%rdi,%r11), %xmm3
	vmovdqu	(%r8,%r11), %xmm5
	incq	%r10
	vpand	.LC0(%rip), %xmm3, %xmm4
	vpand	%xmm1, %xmm3, %xmm3
	vpmulld	%xmm2, %xmm4, %xmm4
	vpxor	%xmm5, %xmm4, %xmm4
	vpand	(%rsi,%r11), %xmm0, %xmm5
	vpor	%xmm5, %xmm3, %xmm3
	vpsrld	$1, %xmm3, %xmm3
	vpxor	%xmm3, %xmm4, %xmm3
	vmovaps	%xmm3, (%r9,%r11)
	addq	$16, %r11
	cmpq	%r10, %rdx
	ja	.L162
	movq	40(%rsp), %rdi
	addq	%rdi, %rbx
	subq	%rdi, %r12
	cmpq	%rdi, 104(%rsp)
	je	.L158
	leaq	1(%rbx), %r9
	leaq	(%rax,%rbx,4), %r11
	movl	(%rax,%r9,4), %r8d
	movl	(%r11), %r10d
	movl	%r8d, %esi
	movl	%r8d, %edi
	andl	$-2147483648, %r10d
	andl	$1, %esi
	andl	$2147483647, %edi
	imull	$-1727483681, %esi, %esi
	xorl	-908(%rax,%rbx,4), %esi
	orl	%r10d, %edi
	shrl	%edi
	xorl	%edi, %esi
	cmpq	$1, %r12
	movl	%esi, (%r11)
	je	.L158
	leaq	2(%rbx), %r11
	andl	$-2147483648, %r8d
	movl	(%rax,%r11,4), %r10d
	movl	%r10d, %esi
	movl	%r10d, %edi
	andl	$1, %esi
	andl	$2147483647, %edi
	imull	$-1727483681, %esi, %esi
	xorl	-904(%rax,%rbx,4), %esi
	orl	%r8d, %edi
	shrl	%edi
	xorl	%edi, %esi
	cmpq	$2, %r12
	movl	%esi, (%rax,%r9,4)
	je	.L158
	movl	12(%rax,%rbx,4), %edi
	andl	$-2147483648, %r10d
	movl	%edi, %esi
	andl	$2147483647, %edi
	andl	$1, %esi
	orl	%r10d, %edi
	imull	$-1727483681, %esi, %esi
	xorl	-900(%rax,%rbx,4), %esi
	shrl	%edi
	xorl	%edi, %esi
	movl	%esi, (%rax,%r11,4)
.L158:
	movl	(%rax), %esi
	movl	2492(%rax), %r9d
	movl	%esi, %edi
	movl	%esi, %r8d
	andl	$-2147483648, %r9d
	andl	$1, %edi
	andl	$2147483647, %r8d
	imull	$-1727483681, %edi, %edi
	xorl	1584(%rax), %edi
	orl	%r9d, %r8d
	shrl	%r8d
	xorl	%r8d, %edi
	movl	$1, %r8d
	movl	%edi, 2492(%rax)
	jmp	.L144
	.p2align 5,,7
	.p2align 3
.L234:
	vsubsd	%xmm3, %xmm6, %xmm0
	vmovsd	%xmm3, 240(%rsp)
	call	log
	vmulsd	.LC6(%rip), %xmm0, %xmm0
	vsqrtsd	%xmm0, %xmm1, %xmm1
	vucomisd	%xmm1, %xmm1
	jp	.L236
.L167:
	vmovsd	.LC7(%rip), %xmm6
	vmovsd	%xmm1, 248(%rsp)
	movb	$1, 256(%rsp)
	vmovsd	%xmm1, 128(%rsp)
	vmulsd	232(%rsp), %xmm6, %xmm0
	call	cos
	vmovsd	128(%rsp), %xmm1
	jmp	.L169
	.p2align 5,,7
	.p2align 3
.L186:
	movl	$222, %r12d
	xorl	%ebx, %ebx
	jmp	.L145
	.p2align 5,,7
	.p2align 3
.L180:
	movl	$222, %ebx
	xorl	%r12d, %r12d
	jmp	.L121
	.p2align 5,,7
	.p2align 3
.L189:
	movl	$396, %r12d
	movl	$227, %ebx
	jmp	.L154
	.p2align 5,,7
	.p2align 3
.L183:
	movl	$396, %ebx
	movl	$227, %r12d
	jmp	.L130
	.p2align 5,,7
	.p2align 3
.L188:
	movl	$220, %r12d
	movl	$2, %ebx
	jmp	.L145
	.p2align 5,,7
	.p2align 3
.L187:
	movl	$221, %r12d
	movl	$1, %ebx
	jmp	.L145
	.p2align 5,,7
	.p2align 3
.L190:
	movl	$395, %r12d
	movl	$228, %ebx
	jmp	.L154
	.p2align 5,,7
	.p2align 3
.L184:
	movl	$395, %ebx
	movl	$228, %r12d
	jmp	.L130
	.p2align 5,,7
	.p2align 3
.L182:
	movl	$220, %ebx
	movl	$2, %r12d
	jmp	.L121
	.p2align 5,,7
	.p2align 3
.L181:
	movl	$221, %ebx
	movl	$1, %r12d
	jmp	.L121
	.p2align 5,,7
	.p2align 3
.L191:
	movl	$394, %r12d
	movl	$229, %ebx
	jmp	.L154
	.p2align 5,,7
	.p2align 3
.L185:
	movl	$394, %ebx
	movl	$229, %r12d
	jmp	.L130
.L170:
	leaq	144(%rsp), %rsi
	leaq	176(%rsp), %rdi
	call	_ZNSt6vectorIdSaIdEE19_M_emplace_back_auxIIdEEEvDpOT_
.LEHE1:
	jmp	.L172
.L236:
	call	sqrt
	vmovapd	%xmm0, %xmm1
	.p2align 4,,4
	jmp	.L167
.L192:
	movq	176(%rsp), %rdi
	movq	%rax, %rbx
	testq	%rdi, %rdi
	je	.L229
	vzeroupper
	call	_ZdlPv
.L178:
	movq	(%rsp), %rdi
	testq	%rdi, %rdi
	je	.L179
	call	_ZdlPv
.L179:
	movq	%rbx, %rdi
.LEHB2:
	call	_Unwind_Resume
.LEHE2:
.L229:
	vzeroupper
	jmp	.L178
	.cfi_endproc
.LFE8861:
	.globl	__gxx_personality_v0
	.section	.gcc_except_table,"a",@progbits
.LLSDA8861:
	.byte	0xff
	.byte	0xff
	.byte	0x1
	.uleb128 .LLSDACSE8861-.LLSDACSB8861
.LLSDACSB8861:
	.uleb128 .LEHB0-.LFB8861
	.uleb128 .LEHE0-.LEHB0
	.uleb128 0
	.uleb128 0
	.uleb128 .LEHB1-.LFB8861
	.uleb128 .LEHE1-.LEHB1
	.uleb128 .L192-.LFB8861
	.uleb128 0
	.uleb128 .LEHB2-.LFB8861
	.uleb128 .LEHE2-.LEHB2
	.uleb128 0
	.uleb128 0
.LLSDACSE8861:
	.section	.text.startup
	.size	main, .-main
	.p2align 5,,31
	.type	_GLOBAL__sub_I_main, @function
_GLOBAL__sub_I_main:
.LFB10183:
	.cfi_startproc
	subq	$8, %rsp
	.cfi_def_cfa_offset 16
	movl	$_ZStL8__ioinit, %edi
	call	_ZNSt8ios_base4InitC1Ev
	movl	$__dso_handle, %edx
	movl	$_ZStL8__ioinit, %esi
	movl	$_ZNSt8ios_base4InitD1Ev, %edi
	call	__cxa_atexit
	cmpb	$0, _ZGVN5boost4math7lanczos19lanczos_initializerINS1_12lanczos17m64EeE11initializerE(%rip)
	jne	.L239
	movb	$1, _ZGVN5boost4math7lanczos19lanczos_initializerINS1_12lanczos17m64EeE11initializerE(%rip)
.L239:
	addq	$8, %rsp
	.cfi_def_cfa_offset 8
	ret
	.cfi_endproc
.LFE10183:
	.size	_GLOBAL__sub_I_main, .-_GLOBAL__sub_I_main
	.section	.init_array,"aw"
	.align 8
	.quad	_GLOBAL__sub_I_main
	.weak	_ZGVN5boost4math7lanczos19lanczos_initializerINS1_12lanczos17m64EeE11initializerE
	.section	.bss._ZGVN5boost4math7lanczos19lanczos_initializerINS1_12lanczos17m64EeE11initializerE,"awG",@nobits,_ZGVN5boost4math7lanczos19lanczos_initializerINS1_12lanczos17m64EeE11initializerE,comdat
	.align 32
	.type	_ZGVN5boost4math7lanczos19lanczos_initializerINS1_12lanczos17m64EeE11initializerE, @gnu_unique_object
	.size	_ZGVN5boost4math7lanczos19lanczos_initializerINS1_12lanczos17m64EeE11initializerE, 8
_ZGVN5boost4math7lanczos19lanczos_initializerINS1_12lanczos17m64EeE11initializerE:
	.zero	8
	.local	_ZStL8__ioinit
	.comm	_ZStL8__ioinit,1,1
	.section	.rodata.cst16,"aM",@progbits,16
	.align 16
.LC0:
	.long	1
	.long	1
	.long	1
	.long	1
	.align 16
.LC1:
	.long	-1727483681
	.long	-1727483681
	.long	-1727483681
	.long	-1727483681
	.align 16
.LC2:
	.long	2147483647
	.long	2147483647
	.long	2147483647
	.long	2147483647
	.align 16
.LC3:
	.long	-2147483648
	.long	-2147483648
	.long	-2147483648
	.long	-2147483648
	.section	.rodata.cst8,"aM",@progbits,8
	.align 8
.LC4:
	.long	0
	.long	1039138816
	.align 8
.LC5:
	.long	0
	.long	1072693248
	.align 8
.LC6:
	.long	0
	.long	-1073741824
	.align 8
.LC7:
	.long	1413754136
	.long	1075388923
	.align 8
.LC10:
	.long	0
	.long	1081733120
	.hidden	__dso_handle
	.ident	"GCC: (GNU) 4.8.2"
	.section	.note.GNU-stack,"",@progbits
